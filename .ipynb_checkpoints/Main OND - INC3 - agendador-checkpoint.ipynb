{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb748e11",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a9b52c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import fnmatch\n",
    "import urllib.request\n",
    "import telegram\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define o número de dias anteriores para buscar as URLs das regiões\n",
    "num_days = 4   #manter em 4 em dias da semana (testar fds)\n",
    "\n",
    "# Obtém as datas dos últimos 'num_days' dias e inverte a lista para buscar da menor para a maior data\n",
    "dates = [datetime.today() - timedelta(days=x) for x in range(num_days)][::-1]\n",
    "\n",
    "# Define os formatos das datas para uso na URL e no nome do arquivo\n",
    "date_formats = [date.strftime('%Y_%m_%d') for date in dates]\n",
    "date_formats2 = [date.strftime('%d-%m-%Y') for date in dates]\n",
    "\n",
    "# Define as URLs de download com variação das datas definidas\n",
    "url_regioes = {}\n",
    "url_prod = {}\n",
    "url_reservatorios= {}\n",
    "\n",
    "for i in range(num_days):\n",
    "    # BALANÇO DE ENERGIA ACUMULADA DIÁRIA - BEA\n",
    "    url_regioes[date_formats[i]] = {\n",
    "        'sul': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/03_DadosDiariosAcumulados_Regiao_{date_formats2[i]}.xlsx',\n",
    "        'sudeste': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/04_DadosDiariosAcumulados_Regiao_{date_formats2[i]}.xlsx',\n",
    "        'nordeste': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/05_DadosDiariosAcumulados_Regiao_{date_formats2[i]}.xlsx',\n",
    "        'norte': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/06_DadosDiariosAcumulados_Regiao_{date_formats2[i]}.xlsx',\n",
    "        'brasil': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/07_DadosDiariosAcumulados_Regiao_{date_formats2[i]}.xlsx'\n",
    "    }\n",
    "    # PRODUÇÃO DE ENERGIA POR TIPO (BDE_D)\n",
    "    url_prod[date_formats[i]] = {\n",
    "        '_H': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/08_ProducaoHidraulicaUsina_{date_formats2[i]}.xlsx',\n",
    "        '_T': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/09_ProducaoTermicaUsina_{date_formats2[i]}.xlsx'\n",
    "        #'PROD_S':PLATAFORMA QUEBRADA\n",
    "        #'PROD_E': PLATAFORMA QUEBRADA\n",
    "    }\n",
    "    \n",
    "    url_reservatorios[date_formats[i]] = {\n",
    "        'sul': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/23_SituacaoPrincipaisReservatorios_Regiao_{date_formats2[i]}.xlsx',\n",
    "        'sudeste': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/24_SituacaoPrincipaisReservatorios_Regiao_{date_formats2[i]}.xlsx',\n",
    "        'nordeste': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/25_SituacaoPrincipaisReservatorios_Regiao_{date_formats2[i]}.xlsx',\n",
    "        'norte': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/26_SituacaoPrincipaisReservatorios_Regiao_{date_formats2[i]}.xlsx'\n",
    "    }\n",
    "    \n",
    "# Percorre URL_regioes para tentar baixar os arquivos\n",
    "for data, urls in url_regioes.items():\n",
    "    for regiao, url in urls.items():\n",
    "        filename = f'BEA_D_{regiao.upper()}_{data}.xlsx'\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "            bot = telegram.Bot(token='SEU_TOKEN_AQUI')\n",
    "            bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} baixado com sucesso')\n",
    "        except urllib.error.HTTPError:\n",
    "            bot = telegram.Bot(token='SEU_TOKEN_AQUI')\n",
    "            bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} não encontrado')\n",
    "            continue\n",
    "\n",
    "# Percorre URL_prod para tentar baixar os arquivos\n",
    "for data, urls in url_prod.items():\n",
    "    for prod_type, url in urls.items():\n",
    "        filename = f'BDE_D_PROD_{prod_type}_{data}.xlsx'\n",
    "        try:      \n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "            bot = telegram.Bot(token='SEU_TOKEN_AQUI')\n",
    "            bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de produção {prod_type} da data {data} baixado com sucesso')\n",
    "        except urllib.error.HTTPError:\n",
    "            bot = telegram.Bot(token='SEU_TOKEN_AQUI')\n",
    "            bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de produção {prod_type} da data {data} não encontrado')\n",
    "            continue\n",
    "    \n",
    "# Percorre URL_regioes para tentar baixar os arquivos\n",
    "for data, urls in url_reservatorios.items():\n",
    "    for reservatorios, url in urls.items():\n",
    "        filename = f'RES_{regiao.upper()}_{data}.xlsx'\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "            bot = telegram.Bot(token='SEU_TOKEN_AQUI')\n",
    "            bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} baixado com sucesso')\n",
    "        except urllib.error.HTTPError:\n",
    "            bot = telegram.Bot(token='SEU_TOKEN_AQUI')\n",
    "            bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} não encontrado')\n",
    "            continue\n",
    "\n",
    "# Percorre URL_reservatorios para tentar baixar os arquivos\n",
    "for data, urls in url_reservatorios.items():\n",
    "    for regiao, url in urls.items():\n",
    "        filename = f'RES_D_{regiao.upper()}_{data}.xlsx'\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "            bot = telegram.Bot(token='SEU_TOKEN_AQUI')\n",
    "            bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} baixado com sucesso')\n",
    "        except urllib.error.HTTPError:\n",
    "            bot = telegram.Bot(token='SEU_TOKEN_AQUI')\n",
    "            bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} não encontrado')\n",
    "            continue\n",
    "            \n",
    "            \n",
    "\n",
    "# ETL BALANÇO DE ENERGIA ACUMULADA DIÁRIA\n",
    "\n",
    "dfs = []\n",
    "for date in date_formats:\n",
    "    df_date = pd.DataFrame()\n",
    "    for regiao in ['sul','sudeste', 'nordeste', 'norte','brasil']:\n",
    "        prod_regiao = f'BEA_D_{regiao.upper()}_{date}.xlsx'\n",
    "        if os.path.exists(prod_regiao):\n",
    "            df = pd.read_excel(prod_regiao, sheet_name='Plan1')\n",
    "            df['regiao'] = regiao\n",
    "            df_date = pd.concat([df_date, df], axis=0)\n",
    "        dfs.append(df_date)\n",
    "        \n",
    "\n",
    "# Concatena e remove col completamente vazias\n",
    "df_BEA_D = pd.concat(dfs, axis=0).dropna()\n",
    "\n",
    "# Renomeia colunas\n",
    "df_BEA_D = df_BEA_D.rename(columns={'Unnamed: 0': 'data',\n",
    "                             'Unnamed: 1': 'total',\n",
    "                             'Unnamed: 2': 'hidraulica',\n",
    "                             'Unnamed: 3': 'termica',\n",
    "                             'Unnamed: 4': 'eolica',\n",
    "                             'Unnamed: 5': 'solar',\n",
    "                             'Unnamed: 6': 'intercambio',\n",
    "                             'Unnamed: 7': 'carga'})\n",
    "\n",
    "# Limpando o dataframe\n",
    "to_remove = ['Dados Diários acumulados', 'Valores - MWmed', 'Subsistema Norte', 'Total', np.nan,'Data', 'Subsistema Sul', 'Subsistema Nordeste', 'Subsistema Sudeste']\n",
    "df_BEA_D = df_BEA_D[~df_BEA_D['data'].isin(to_remove)]\n",
    "\n",
    "# Remove valores que se repetem em 'data' e 'regiao', incluindo valores vazios\n",
    "df_BEA_D.drop_duplicates(subset=['data', 'regiao'], inplace=True)\n",
    "\n",
    "#atualizando formato dos dados\n",
    "df_BEA_D['total'] = pd.to_numeric(df_BEA_D['total'], errors='coerce')\n",
    "df_BEA_D['hidraulica'] = pd.to_numeric(df_BEA_D['hidraulica'], errors='coerce')\n",
    "df_BEA_D['termica'] = pd.to_numeric(df_BEA_D['termica'], errors='coerce')\n",
    "df_BEA_D['eolica'] = pd.to_numeric(df_BEA_D['eolica'], errors='coerce')\n",
    "df_BEA_D['solar'] = pd.to_numeric(df_BEA_D['solar'], errors='coerce')\n",
    "df_BEA_D['intercambio'] = pd.to_numeric(df_BEA_D['intercambio'], errors='coerce')\n",
    "df_BEA_D['carga'] = pd.to_numeric(df_BEA_D['carga'], errors='coerce')\n",
    "df_BEA_D['data'] = pd.to_datetime(df_BEA_D['data'], format='%d/%m/%Y')\n",
    "\n",
    "\n",
    "\n",
    "# ETL prod, pot e usina (H e T)\n",
    "\n",
    "\n",
    "# Obtém o caminho atual do diretório do script\n",
    "dir_path = os.getcwd()\n",
    "\n",
    "# Define o padrão de nome de arquivo para correspondência\n",
    "filename_pattern_h = 'BDE_D_PROD__H*.xlsx'\n",
    "filename_pattern_t = 'BDE_D_PROD__T*.xlsx'\n",
    "\n",
    "# Cria listas vazias para armazenar os DataFrames de cada tipo de arquivo\n",
    "dfs_h1 = []\n",
    "dfs_h2 = []\n",
    "dfs_h3 = []\n",
    "dfs_t1 = []\n",
    "dfs_t2 = []\n",
    "dfs_t3 = []\n",
    "\n",
    "# Percorre todos os arquivos na pasta atual que correspondem ao padrão de nome de arquivo\n",
    "for filename in os.listdir(dir_path):\n",
    "    if fnmatch.fnmatch(filename, filename_pattern_h):\n",
    "        try:\n",
    "            # Extrai a data do nome do arquivo\n",
    "            date_str = '_'.join(filename.split('_')[5:9]).split('.')[0]\n",
    "            \n",
    "            # Tenta converter a string em um objeto datetime\n",
    "            try:\n",
    "                date_obj = datetime.datetime.strptime(date_str, '%Y_%m_%d')\n",
    "                \n",
    "                # Lê as tabelas prodh_rg, poth_rg e prodh_usina em seus respectivos dataframes e adiciona uma coluna com a data\n",
    "                prodh_rg = pd.read_excel(os.path.join(dir_path, filename), sheet_name='Plan1', header=3, usecols='A:D', nrows=7)\n",
    "                prodh_rg['data'] = date_obj\n",
    "                dfs_h1.append(prodh_rg)\n",
    "                \n",
    "                poth_rg = pd.read_excel(os.path.join(dir_path, filename), sheet_name='Plan1', header=11, usecols='A:D', nrows=7)\n",
    "                poth_rg['data'] = date_obj\n",
    "                dfs_h2.append(poth_rg)\n",
    "                \n",
    "                prodh_usina = pd.read_excel(os.path.join(dir_path, filename), sheet_name='Plan1', header=22, usecols='A:D', nrows=1000)\n",
    "                prodh_usina['data'] = date_obj\n",
    "                dfs_h3.append(prodh_usina)\n",
    "            except ValueError:\n",
    "                # A string não pode ser convertida em uma data\n",
    "                print(f\"Valor inválido na coluna date do arquivo {filename}: {date_str}\")\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler o arquivo {filename}: {e}\")\n",
    "    \n",
    "    elif fnmatch.fnmatch(filename, filename_pattern_t):\n",
    "        try:\n",
    "            # Extrai a data do nome do arquivo\n",
    "            date_str = '_'.join(filename.split('_')[5:9]).split('.')[0]\n",
    "            \n",
    "            # Tenta converter a string em um objeto datetime\n",
    "            try:\n",
    "                date_obj = datetime.datetime.strptime(date_str, '%Y_%m_%d')\n",
    "                \n",
    "                # Lê as tabelas prodt_rg, pott_rg e prodt_usina em seus respectivos dataframes e adiciona uma coluna com a data\n",
    "                prodt_rg = pd.read_excel(os.path.join(dir_path, filename), sheet_name='Plan1', header=3, usecols='A:D', nrows=7)\n",
    "                prodt_rg['data'] = date_obj\n",
    "                dfs_t1.append(prodt_rg)\n",
    "                \n",
    "                pott_rg = pd.read_excel(os.path.join(dir_path, filename), sheet_name='Plan1', header=10, usecols='A:D', nrows=7)\n",
    "                pott_rg['data'] = date_obj\n",
    "                dfs_t2.append(pott_rg)\n",
    "                \n",
    "                prodt_usina = pd.read_excel(os.path.join(dir_path, filename), sheet_name='Plan1', header=20, usecols='A:D', nrows=1000)\n",
    "                prodt_usina['data'] = date_obj\n",
    "                dfs_t3.append(prodh_usina)\n",
    "            except ValueError:\n",
    "                # A string não pode ser convertida em uma data\n",
    "                print(f\"Valor inválido na coluna date do arquivo {filename}: {date_str}\")\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler o arquivo {filename}: {e}\")\n",
    "        \n",
    "# OBS: a tabela BDE_D_PROD__H Itaipu é classificada como uma região, criando +1 linha de diferença por tabela à BDE_D_PROD__T \n",
    "\n",
    "# Concatenando os dataframes Hidráulico Térmico\n",
    "df_prodh_rg = pd.concat(dfs_h1)\n",
    "df_poth_rg = pd.concat(dfs_h2)\n",
    "df_prodh_usina = pd.concat(dfs_h3)\n",
    "df_prodt_rg = pd.concat(dfs_t1)\n",
    "df_pott_rg = pd.concat(dfs_t2)\n",
    "df_prodt_usina = pd.concat(dfs_t3)\n",
    "\n",
    "\n",
    "# A tabela df_prodt_rg apresentou valores em object mesmo a leitura correta\n",
    "df_prodt_rg['GWh no Dia'] = pd.to_numeric(df_prodt_rg['GWh no Dia'], errors='coerce')\n",
    "df_prodt_rg['GWh acum. no Mês até o Dia'] = pd.to_numeric(df_prodt_rg['GWh acum. no Mês até o Dia'], errors='coerce')\n",
    "df_prodt_rg['GWh acum. no Ano até o Dia'] = pd.to_numeric(df_prodt_rg['GWh acum. no Ano até o Dia'], errors='coerce')\n",
    "\n",
    "\n",
    "# Removendo linhas duplicadas\n",
    "dfs = [df_prodh_rg, df_poth_rg, df_prodh_usina, df_prodt_rg, df_pott_rg, df_prodt_usina]\n",
    "dfs = list(map(lambda df: df.drop_duplicates(), dfs))\n",
    "\n",
    "\n",
    "# Normatizando nome das colunas para criar relações no PostgreSQL\n",
    "df_prodh_rg = df_prodh_rg.rename(columns={'Subsistema': 'subsistema',\n",
    "                          'GWh no Dia': 'gwmed_dia',\n",
    "                          'GWh acum. no Mês até o Dia': 'gwmed_mes',\n",
    "                          'GWh acum. no Ano até o Dia': 'gwmed_ano'})\n",
    "\n",
    "df_prodt_rg = df_prodt_rg.rename(columns={'Subsistema': 'subsistema',\n",
    "                          'GWh no Dia': 'gwmed_dia',\n",
    "                          'GWh acum. no Mês até o Dia': 'gwmed_mes',\n",
    "                          'GWh acum. no Ano até o Dia': 'gwmed_ano'})\n",
    "\n",
    "df_poth_rg = df_poth_rg.rename(columns={'Subsistema': 'subsistema',\n",
    "                          'MWmed no Dia': 'mwmed_dia',\n",
    "                          'MWmed no Mês até o Dia': 'mwmed_mes',\n",
    "                          'MWmed no Ano até o Dia': 'mwmed_ano'})\n",
    "\n",
    "df_pott_rg = df_pott_rg.rename(columns={'Subsistema': 'subsistema',\n",
    "                          'MWmed no Dia': 'mwmed_dia',\n",
    "                          'MWmed no Mês até o Dia': 'mwmed_mes',\n",
    "                          'MWmed no Ano até o Dia': 'mwmed_ano'})\n",
    "\n",
    "df_prodt_usina = df_prodt_usina.rename(columns={'Usina': 'usina',\n",
    "                          'Código ONS': 'codigo_ons',\n",
    "                          'Programado (MWmed)': 'programado',\n",
    "                          'Verificado (MWmed)': 'verificado'})\n",
    "\n",
    "df_prodh_usina = df_prodh_usina.rename(columns={'Usina': 'usina',\n",
    "                          'Código ONS': 'codigo_ons',\n",
    "                          'Programado (MWmed)': 'programado',\n",
    "                          'Verificado (MWmed)': 'verificado'})\n",
    "\n",
    "\n",
    "# ELT reservatorios\n",
    "import pandas as pd\n",
    "\n",
    "dfs = []\n",
    "for date in date_formats:\n",
    "    df_date = pd.DataFrame()\n",
    "    for regiao in ['sul','sudeste', 'nordeste', 'norte']:\n",
    "        cap_reservatorio = f'RES_D_{regiao.upper()}_{date}.xlsx'\n",
    "        if os.path.exists(cap_reservatorio):\n",
    "            df = pd.read_excel(cap_reservatorio, sheet_name='Plan1')\n",
    "            filename = os.path.basename(cap_reservatorio)\n",
    "            date_str = '_'.join(filename.split('_')[3:6]).split('.')[0]\n",
    "            df['data'] = date_str\n",
    "            df['regiao'] = regiao\n",
    "            df_date = pd.concat([df_date, df], axis=0)\n",
    "    dfs.append(df_date)\n",
    "\n",
    "# Concatena e remove col completamente vazias\n",
    "df_RES_D = pd.concat(dfs, axis=0)\n",
    "\n",
    "# Renomeia colunas\n",
    "df_RES_D = df_RES_D.rename(columns={'Unnamed: 0': 'bacia',\n",
    "                             'Unnamed: 1': 'reservatorio',\n",
    "                             'Unnamed: 2': 'del',\n",
    "                             'Unnamed: 3': 'nivel_m',\n",
    "                             'Unnamed: 4': 'vol_util_%',\n",
    "                             'Unnamed: 5': 'afluencia',\n",
    "                             'Unnamed: 6': 'defluencia',\n",
    "                             'Unnamed: 7': 'vertida',\n",
    "                             'Unnamed: 8': 'transferida'\n",
    "                                   })\n",
    "\n",
    "# remover valores duplicados na lista to_remove\n",
    "to_remove = list(set(['Dados Hidráulicos dos Reservatórios', 'Subsistema Sul', 'Subsistema Norte', 'Bacia', 'Subsistema Sul', 'Subsistema Nordeste', 'Subsistema Sudeste']))\n",
    "df_RES_D.drop('del', axis=1, inplace=True)\n",
    "\n",
    "# remover linhas do dataframe que contenham valores presentes em to_remove na coluna col_name\n",
    "df_RES_D = df_RES_D[~df_RES_D['bacia'].isin(to_remove)]\n",
    "df_RES_D = df_RES_D[df_RES_D['afluencia'] != 'Afluência']\n",
    "df_RES_D.drop_duplicates(subset=['reservatorio', 'data'], inplace=True)\n",
    "\n",
    "# Corrige falores da col 'bacia' que estavam mesclados no excel\n",
    "df_RES_D['bacia'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "#atualizando formato dos dados\n",
    "df_RES_D['nivel_m'] = pd.to_numeric(df_RES_D['nivel_m'], errors='coerce')\n",
    "df_RES_D['vol_util_%'] = pd.to_numeric(df_RES_D['vol_util_%'], errors='coerce')\n",
    "df_RES_D['afluencia'] = pd.to_numeric(df_RES_D['afluencia'], errors='coerce')\n",
    "df_RES_D['defluencia'] = pd.to_numeric(df_RES_D['defluencia'], errors='coerce')\n",
    "df_RES_D['vertida'] = pd.to_numeric(df_RES_D['vertida'], errors='coerce')\n",
    "df_RES_D['transferida'] = pd.to_numeric(df_RES_D['transferida'], errors='coerce')\n",
    "df_RES_D['data'] = pd.to_datetime(df_RES_D['data'],  format='%Y_%m_%d')\n",
    "\n",
    "\n",
    "# Conecta-se ao banco de dados local\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import psycopg2 as pg\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Cria conexão com o banco de dados por engine\n",
    "engine = create_engine('postgresql://postgres:gabriel123@localhost:5432/ONS_D')\n",
    "\n",
    "# Cria conexão com o banco por pycopg2\n",
    "connection = pg.connect(user = \"postgres\",\n",
    "                          password = \"gabriel123\",\n",
    "                          host = \"localhost\",\n",
    "                          port = \"5432\",\n",
    "                          database = \"ONS_D\")\n",
    "curs = connection.cursor()\n",
    "\n",
    "\n",
    "# Subindo dataframes gerados em Python para o banco ONS_D\n",
    "\n",
    "df_BEA_D.to_sql('balanco_de_energia', engine, if_exists='replace', index=False)\n",
    "\n",
    "df_pott_rg.to_sql('potencia_termicas', engine, if_exists='replace', index=False)\n",
    "df_poth_rg.to_sql('potencia_hidraulicas', engine, if_exists='replace', index=False)\n",
    "\n",
    "df_prodt_rg.to_sql('producao_termicas', engine, if_exists='replace', index=False)\n",
    "df_prodh_rg.to_sql('producao_hidraulicas', engine, if_exists='replace', index=False)\n",
    "\n",
    "df_prodh_usina.to_sql('usinas_hidraulicas', engine, if_exists='replace', index=False)\n",
    "df_prodt_usina.to_sql('usinas_termicas', engine, if_exists='replace', index=False)\n",
    "\n",
    "df_RES_D.to_sql('reservatorios', engine, if_exists='replace', index=False)\n",
    "\n",
    "\n",
    "\n",
    "# ETL no ONS_D  -  prevendo manipulações futuras realizei as operações individualmente\n",
    "\n",
    "# Remove linhas duplicadas no SQL - balanco_de_energia\n",
    "engine.execute(\"\"\"\n",
    "    DELETE FROM balanco_de_energia\n",
    "    WHERE ctid NOT IN (\n",
    "        SELECT DISTINCT ON (data, total, hidraulica, termica, eolica, solar, intercambio, carga, regiao) ctid\n",
    "        FROM balanco_de_energia\n",
    "        ORDER BY data, total, hidraulica, termica, eolica, solar, intercambio, carga, regiao, ctid\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Remove linhas duplicadas no SQL - potencia\n",
    "engine.execute(\"\"\"\n",
    "    DELETE FROM potencia_termicas\n",
    "    WHERE ctid NOT IN (\n",
    "        SELECT DISTINCT ON (subsistema, mwmed_dia) ctid\n",
    "        FROM potencia_termicas\n",
    "        WHERE subsistema != 'None'\n",
    "        ORDER BY subsistema, mwmed_dia, data, ctid\n",
    "    )\n",
    "\"\"\")\n",
    "engine.execute(\"\"\"\n",
    "    DELETE FROM potencia_hidraulicas\n",
    "    WHERE ctid NOT IN (\n",
    "        SELECT DISTINCT ON (subsistema, mwmed_dia) ctid\n",
    "        FROM potencia_hidraulicas\n",
    "        WHERE subsistema != 'None'\n",
    "        ORDER BY subsistema, mwmed_dia, data, ctid\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Remove linhas duplicadas no SQL - producao\n",
    "engine.execute(\"\"\"\n",
    "    DELETE FROM producao_termicas\n",
    "    WHERE ctid NOT IN (\n",
    "        SELECT DISTINCT ON (subsistema, gwmed_dia) ctid\n",
    "        FROM producao_termicas\n",
    "        WHERE subsistema != 'Subsistema'\n",
    "        ORDER BY subsistema, gwmed_dia, data, ctid\n",
    "    )\n",
    "\"\"\") \n",
    "engine.execute(\"\"\"\n",
    "    DELETE FROM producao_hidraulicas\n",
    "    WHERE ctid NOT IN (\n",
    "        SELECT DISTINCT ON (subsistema, gwmed_dia) ctid\n",
    "        FROM producao_hidraulicas\n",
    "        WHERE subsistema != 'Subsistema'\n",
    "        ORDER BY subsistema, gwmed_dia, data, ctid\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Remove linhas duplicadas no SQL - usinas\n",
    "engine.execute(\"\"\"\n",
    "    DELETE FROM usinas_hidraulicas\n",
    "    WHERE ctid NOT IN (\n",
    "        SELECT DISTINCT ON (codigo_ons, data) ctid\n",
    "        FROM usinas_hidraulicas\n",
    "        ORDER BY codigo_ons, data, ctid\n",
    "    )\n",
    "\"\"\") \n",
    "engine.execute(\"\"\"\n",
    "    DELETE FROM usinas_termicas\n",
    "    WHERE ctid NOT IN (\n",
    "        SELECT DISTINCT ON (codigo_ons, data) ctid\n",
    "        FROM usinas_termicas\n",
    "        ORDER BY codigo_ons, data, ctid\n",
    "    )\n",
    "\"\"\") \n",
    "\n",
    "# Remove linhas duplicadas no SQL - reservatorios\n",
    "engine.execute(\"\"\"\n",
    "    DELETE FROM reservatorios\n",
    "    WHERE ctid NOT IN (\n",
    "        SELECT DISTINCT ON (reservatorio, nivel_m) ctid\n",
    "        FROM reservatorios\n",
    "        WHERE reservatorio != 'None'\n",
    "        ORDER BY reservatorio, nivel_m, ctid\n",
    "    )\n",
    "\"\"\") \n",
    "\n",
    "# Limpa a pasta de download\n",
    "\n",
    "import os\n",
    "\n",
    "# Define o diretório onde os arquivos foram salvos\n",
    "diretorio = r'C:\\Users\\ghumb\\Desktop\\Python\\Projetos\\ONS'\n",
    "\n",
    "# Percorre todos os arquivos no diretório e exclui aqueles cujo nome começa com BDE, BEA ou RES e têm extensão .xlsx\n",
    "for filename in os.listdir(diretorio):\n",
    "    if (filename.startswith('BDE') or filename.startswith('BEA') or filename.startswith('RES')):\n",
    "        os.remove(os.path.join(diretorio, filename))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876bed7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
