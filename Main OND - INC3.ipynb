{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb748e11",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3672d03",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1387124377.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\ghumb\\AppData\\Local\\Temp\\ipykernel_25044\\1387124377.py\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    balanco_de_energia = df_BEA_D\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Tabelas disponíveis (PostgreSQL = py)\n",
    "\n",
    "    # BALANÇO DE ENERGIA ACUMULADA DIÁRIA \n",
    "        balanco_de_energia = df_BEA_D\n",
    "\n",
    "    # PRODUÇÃO DE ENERGIA POR TIPO\n",
    "        # Hidráulica\n",
    "        producao_hidraulicas = df_prodh_rg - produção diária por região + itaipu\n",
    "        potencia_hidraulicas = df_poth_rg - potencia diária por região + itaipu\n",
    "        usinas_hidraulicas = df_prodh_usina - produção e potencia diária por usina + itaipu\n",
    "        # Térmica\n",
    "        producao_termicas = df_prodt_rg - produção diária por região\n",
    "        potencia_termicas = df_pott_rg - potencia diária por região\n",
    "        usinas_termicas = df_prodt_usina -produção e potencia diária por unidade geradora\n",
    "        # Eólica\n",
    "        # Solar\n",
    "    \n",
    "    # Dados dos reservatórios\n",
    "        reservatorios = df_RES_D\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da4512",
   "metadata": {},
   "source": [
    "Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a9b52c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import fnmatch\n",
    "import urllib.request\n",
    "import telegram\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25abc23c",
   "metadata": {},
   "source": [
    "Download de tabelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33bcf06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ghumb\\AppData\\Local\\Temp\\ipykernel_24804\\1863326716.py:47: RuntimeWarning: coroutine 'Bot.send_message' was never awaited\n",
      "  bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} baixado com sucesso')\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\ghumb\\AppData\\Local\\Temp\\ipykernel_24804\\1863326716.py:50: RuntimeWarning: coroutine 'Bot.send_message' was never awaited\n",
      "  bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} não encontrado')\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\ghumb\\AppData\\Local\\Temp\\ipykernel_24804\\1863326716.py:60: RuntimeWarning: coroutine 'Bot.send_message' was never awaited\n",
      "  bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de produção {prod_type} da data {data} baixado com sucesso')\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\ghumb\\AppData\\Local\\Temp\\ipykernel_24804\\1863326716.py:63: RuntimeWarning: coroutine 'Bot.send_message' was never awaited\n",
      "  bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de produção {prod_type} da data {data} não encontrado')\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\ghumb\\AppData\\Local\\Temp\\ipykernel_24804\\1863326716.py:73: RuntimeWarning: coroutine 'Bot.send_message' was never awaited\n",
      "  bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} baixado com sucesso')\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\ghumb\\AppData\\Local\\Temp\\ipykernel_24804\\1863326716.py:76: RuntimeWarning: coroutine 'Bot.send_message' was never awaited\n",
      "  bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} não encontrado')\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\ghumb\\AppData\\Local\\Temp\\ipykernel_24804\\1863326716.py:86: RuntimeWarning: coroutine 'Bot.send_message' was never awaited\n",
      "  bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} baixado com sucesso')\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\ghumb\\AppData\\Local\\Temp\\ipykernel_24804\\1863326716.py:89: RuntimeWarning: coroutine 'Bot.send_message' was never awaited\n",
      "  bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} não encontrado')\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define o número de dias anteriores para buscar as URLs das regiões\n",
    "num_days = 4   #manter em 4 em dias da semana (testar fds)\n",
    "\n",
    "# Obtém as datas dos últimos 'num_days' dias e inverte a lista para buscar da menor para a maior data\n",
    "dates = [datetime.today() - timedelta(days=x) for x in range(num_days)][::-1]\n",
    "\n",
    "# Define os formatos das datas para uso na URL e no nome do arquivo\n",
    "date_formats = [date.strftime('%Y_%m_%d') for date in dates]\n",
    "date_formats2 = [date.strftime('%d-%m-%Y') for date in dates]\n",
    "\n",
    "# Define as URLs de download com variação das datas definidas\n",
    "url_regioes = {}\n",
    "url_prod = {}\n",
    "url_reservatorios= {}\n",
    "\n",
    "for i in range(num_days):\n",
    "    # BALANÇO DE ENERGIA ACUMULADA DIÁRIA - BEA\n",
    "    url_regioes[date_formats[i]] = {\n",
    "        'sul': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/03_DadosDiariosAcumulados_Regiao_{date_formats2[i]}.xlsx',\n",
    "        'sudeste': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/04_DadosDiariosAcumulados_Regiao_{date_formats2[i]}.xlsx',\n",
    "        'nordeste': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/05_DadosDiariosAcumulados_Regiao_{date_formats2[i]}.xlsx',\n",
    "        'norte': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/06_DadosDiariosAcumulados_Regiao_{date_formats2[i]}.xlsx',\n",
    "        'brasil': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/07_DadosDiariosAcumulados_Regiao_{date_formats2[i]}.xlsx'\n",
    "    }\n",
    "    # PRODUÇÃO DE ENERGIA POR TIPO (BDE_D)\n",
    "    url_prod[date_formats[i]] = {\n",
    "        '_H': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/08_ProducaoHidraulicaUsina_{date_formats2[i]}.xlsx',\n",
    "        '_T': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/09_ProducaoTermicaUsina_{date_formats2[i]}.xlsx'\n",
    "        #'PROD_S':PLATAFORMA QUEBRADA\n",
    "        #'PROD_E': PLATAFORMA QUEBRADA\n",
    "    }\n",
    "    \n",
    "    url_reservatorios[date_formats[i]] = {\n",
    "        'sul': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/23_SituacaoPrincipaisReservatorios_Regiao_{date_formats2[i]}.xlsx',\n",
    "        'sudeste': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/24_SituacaoPrincipaisReservatorios_Regiao_{date_formats2[i]}.xlsx',\n",
    "        'nordeste': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/25_SituacaoPrincipaisReservatorios_Regiao_{date_formats2[i]}.xlsx',\n",
    "        'norte': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/26_SituacaoPrincipaisReservatorios_Regiao_{date_formats2[i]}.xlsx'\n",
    "    }\n",
    "    \n",
    "# Percorre URL_regioes para tentar baixar os arquivos\n",
    "for data, urls in url_regioes.items():\n",
    "    for regiao, url in urls.items():\n",
    "        filename = f'BEA_D_{regiao.upper()}_{data}.xlsx'\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "            bot = telegram.Bot(token='SEU_TOKEN_AQUI')\n",
    "            bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} baixado com sucesso')\n",
    "        except urllib.error.HTTPError:\n",
    "            bot = telegram.Bot(token='SEU_TOKEN_AQUI')\n",
    "            bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} não encontrado')\n",
    "            continue\n",
    "\n",
    "# Percorre URL_prod para tentar baixar os arquivos\n",
    "for data, urls in url_prod.items():\n",
    "    for prod_type, url in urls.items():\n",
    "        filename = f'BDE_D_PROD_{prod_type}_{data}.xlsx'\n",
    "        try:      \n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "            bot = telegram.Bot(token='SEU_TOKEN_AQUI')\n",
    "            bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de produção {prod_type} da data {data} baixado com sucesso')\n",
    "        except urllib.error.HTTPError:\n",
    "            bot = telegram.Bot(token='SEU_TOKEN_AQUI')\n",
    "            bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de produção {prod_type} da data {data} não encontrado')\n",
    "            continue\n",
    "    \n",
    "# Percorre URL_regioes para tentar baixar os arquivos\n",
    "for data, urls in url_reservatorios.items():\n",
    "    for reservatorios, url in urls.items():\n",
    "        filename = f'RES_{regiao.upper()}_{data}.xlsx'\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "            bot = telegram.Bot(token='SEU_TOKEN_AQUI')\n",
    "            bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} baixado com sucesso')\n",
    "        except urllib.error.HTTPError:\n",
    "            bot = telegram.Bot(token='SEU_TOKEN_AQUI')\n",
    "            bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} não encontrado')\n",
    "            continue\n",
    "\n",
    "# Percorre URL_reservatorios para tentar baixar os arquivos\n",
    "for data, urls in url_reservatorios.items():\n",
    "    for regiao, url in urls.items():\n",
    "        filename = f'RES_D_{regiao.upper()}_{data}.xlsx'\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "            bot = telegram.Bot(token='SEU_TOKEN_AQUI')\n",
    "            bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} baixado com sucesso')\n",
    "        except urllib.error.HTTPError:\n",
    "            bot = telegram.Bot(token='SEU_TOKEN_AQUI')\n",
    "            bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} não encontrado')\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebc809b",
   "metadata": {},
   "source": [
    "ETLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebe08d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL BALANÇO DE ENERGIA ACUMULADA DIÁRIA\n",
    "\n",
    "dfs = []\n",
    "for date in date_formats:\n",
    "    df_date = pd.DataFrame()\n",
    "    for regiao in ['sul','sudeste', 'nordeste', 'norte','brasil']:\n",
    "        prod_regiao = f'BEA_D_{regiao.upper()}_{date}.xlsx'\n",
    "        if os.path.exists(prod_regiao):\n",
    "            df = pd.read_excel(prod_regiao, sheet_name='Plan1')\n",
    "            df['regiao'] = regiao\n",
    "            df_date = pd.concat([df_date, df], axis=0)\n",
    "        dfs.append(df_date)\n",
    "        \n",
    "\n",
    "# Concatena e remove col completamente vazias\n",
    "df_BEA_D = pd.concat(dfs, axis=0).dropna()\n",
    "\n",
    "# Renomeia colunas\n",
    "df_BEA_D = df_BEA_D.rename(columns={'Unnamed: 0': 'data',\n",
    "                             'Unnamed: 1': 'total',\n",
    "                             'Unnamed: 2': 'hidraulica',\n",
    "                             'Unnamed: 3': 'termica',\n",
    "                             'Unnamed: 4': 'eolica',\n",
    "                             'Unnamed: 5': 'solar',\n",
    "                             'Unnamed: 6': 'intercambio',\n",
    "                             'Unnamed: 7': 'carga'})\n",
    "\n",
    "# Limpando o dataframe\n",
    "to_remove = ['Dados Diários acumulados', 'Valores - MWmed', 'Subsistema Norte', 'Total', np.nan,'Data', 'Subsistema Sul', 'Subsistema Nordeste', 'Subsistema Sudeste']\n",
    "df_BEA_D = df_BEA_D[~df_BEA_D['data'].isin(to_remove)]\n",
    "\n",
    "# Remove valores que se repetem em 'data' e 'regiao', incluindo valores vazios\n",
    "df_BEA_D.drop_duplicates(subset=['data', 'regiao'], inplace=True)\n",
    "\n",
    "#atualizando formato dos dados\n",
    "df_BEA_D['total'] = pd.to_numeric(df_BEA_D['total'], errors='coerce')\n",
    "df_BEA_D['hidraulica'] = pd.to_numeric(df_BEA_D['hidraulica'], errors='coerce')\n",
    "df_BEA_D['termica'] = pd.to_numeric(df_BEA_D['termica'], errors='coerce')\n",
    "df_BEA_D['eolica'] = pd.to_numeric(df_BEA_D['eolica'], errors='coerce')\n",
    "df_BEA_D['solar'] = pd.to_numeric(df_BEA_D['solar'], errors='coerce')\n",
    "df_BEA_D['intercambio'] = pd.to_numeric(df_BEA_D['intercambio'], errors='coerce')\n",
    "df_BEA_D['carga'] = pd.to_numeric(df_BEA_D['carga'], errors='coerce')\n",
    "df_BEA_D['data'] = pd.to_datetime(df_BEA_D['data'], format='%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "838dceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL prod, pot e usina (H e T)\n",
    "\n",
    "\n",
    "# Obtém o caminho atual do diretório do script\n",
    "dir_path = os.getcwd()\n",
    "\n",
    "# Define o padrão de nome de arquivo para correspondência\n",
    "filename_pattern_h = 'BDE_D_PROD__H*.xlsx'\n",
    "filename_pattern_t = 'BDE_D_PROD__T*.xlsx'\n",
    "\n",
    "# Cria listas vazias para armazenar os DataFrames de cada tipo de arquivo\n",
    "dfs_h1 = []\n",
    "dfs_h2 = []\n",
    "dfs_h3 = []\n",
    "dfs_t1 = []\n",
    "dfs_t2 = []\n",
    "dfs_t3 = []\n",
    "\n",
    "# Percorre todos os arquivos na pasta atual que correspondem ao padrão de nome de arquivo\n",
    "for filename in os.listdir(dir_path):\n",
    "    if fnmatch.fnmatch(filename, filename_pattern_h):\n",
    "        try:\n",
    "            # Extrai a data do nome do arquivo\n",
    "            date_str = '_'.join(filename.split('_')[5:9]).split('.')[0]\n",
    "            \n",
    "            # Tenta converter a string em um objeto datetime\n",
    "            try:\n",
    "                date_obj = datetime.datetime.strptime(date_str, '%Y_%m_%d')\n",
    "                \n",
    "                # Lê as tabelas prodh_rg, poth_rg e prodh_usina em seus respectivos dataframes e adiciona uma coluna com a data\n",
    "                prodh_rg = pd.read_excel(os.path.join(dir_path, filename), sheet_name='Plan1', header=3, usecols='A:D', nrows=7)\n",
    "                prodh_rg['data'] = date_obj\n",
    "                dfs_h1.append(prodh_rg)\n",
    "                \n",
    "                poth_rg = pd.read_excel(os.path.join(dir_path, filename), sheet_name='Plan1', header=11, usecols='A:D', nrows=7)\n",
    "                poth_rg['data'] = date_obj\n",
    "                dfs_h2.append(poth_rg)\n",
    "                \n",
    "                prodh_usina = pd.read_excel(os.path.join(dir_path, filename), sheet_name='Plan1', header=22, usecols='A:D', nrows=1000)\n",
    "                prodh_usina['data'] = date_obj\n",
    "                dfs_h3.append(prodh_usina)\n",
    "            except ValueError:\n",
    "                # A string não pode ser convertida em uma data\n",
    "                print(f\"Valor inválido na coluna date do arquivo {filename}: {date_str}\")\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler o arquivo {filename}: {e}\")\n",
    "    \n",
    "    elif fnmatch.fnmatch(filename, filename_pattern_t):\n",
    "        try:\n",
    "            # Extrai a data do nome do arquivo\n",
    "            date_str = '_'.join(filename.split('_')[5:9]).split('.')[0]\n",
    "            \n",
    "            # Tenta converter a string em um objeto datetime\n",
    "            try:\n",
    "                date_obj = datetime.datetime.strptime(date_str, '%Y_%m_%d')\n",
    "                \n",
    "                # Lê as tabelas prodt_rg, pott_rg e prodt_usina em seus respectivos dataframes e adiciona uma coluna com a data\n",
    "                prodt_rg = pd.read_excel(os.path.join(dir_path, filename), sheet_name='Plan1', header=3, usecols='A:D', nrows=7)\n",
    "                prodt_rg['data'] = date_obj\n",
    "                dfs_t1.append(prodt_rg)\n",
    "                \n",
    "                pott_rg = pd.read_excel(os.path.join(dir_path, filename), sheet_name='Plan1', header=10, usecols='A:D', nrows=7)\n",
    "                pott_rg['data'] = date_obj\n",
    "                dfs_t2.append(pott_rg)\n",
    "                \n",
    "                prodt_usina = pd.read_excel(os.path.join(dir_path, filename), sheet_name='Plan1', header=20, usecols='A:D', nrows=1000)\n",
    "                prodt_usina['data'] = date_obj\n",
    "                dfs_t3.append(prodh_usina)\n",
    "            except ValueError:\n",
    "                # A string não pode ser convertida em uma data\n",
    "                print(f\"Valor inválido na coluna date do arquivo {filename}: {date_str}\")\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler o arquivo {filename}: {e}\")\n",
    "        \n",
    "# OBS: a tabela BDE_D_PROD__H Itaipu é classificada como uma região, criando +1 linha de diferença por tabela à BDE_D_PROD__T \n",
    "\n",
    "# Concatenando os dataframes Hidráulico Térmico\n",
    "df_prodh_rg = pd.concat(dfs_h1)\n",
    "df_poth_rg = pd.concat(dfs_h2)\n",
    "df_prodh_usina = pd.concat(dfs_h3)\n",
    "df_prodt_rg = pd.concat(dfs_t1)\n",
    "df_pott_rg = pd.concat(dfs_t2)\n",
    "df_prodt_usina = pd.concat(dfs_t3)\n",
    "\n",
    "\n",
    "# A tabela df_prodt_rg apresentou valores em object mesmo a leitura correta\n",
    "df_prodt_rg['GWh no Dia'] = pd.to_numeric(df_prodt_rg['GWh no Dia'], errors='coerce')\n",
    "df_prodt_rg['GWh acum. no Mês até o Dia'] = pd.to_numeric(df_prodt_rg['GWh acum. no Mês até o Dia'], errors='coerce')\n",
    "df_prodt_rg['GWh acum. no Ano até o Dia'] = pd.to_numeric(df_prodt_rg['GWh acum. no Ano até o Dia'], errors='coerce')\n",
    "\n",
    "\n",
    "# Removendo linhas duplicadas\n",
    "dfs = [df_prodh_rg, df_poth_rg, df_prodh_usina, df_prodt_rg, df_pott_rg, df_prodt_usina]\n",
    "dfs = list(map(lambda df: df.drop_duplicates(), dfs))\n",
    "\n",
    "\n",
    "# Normatizando nome das colunas para criar relações no PostgreSQL\n",
    "df_prodh_rg = df_prodh_rg.rename(columns={'Subsistema': 'subsistema',\n",
    "                          'GWh no Dia': 'gwmed_dia',\n",
    "                          'GWh acum. no Mês até o Dia': 'gwmed_mes',\n",
    "                          'GWh acum. no Ano até o Dia': 'gwmed_ano'})\n",
    "\n",
    "df_prodt_rg = df_prodt_rg.rename(columns={'Subsistema': 'subsistema',\n",
    "                          'GWh no Dia': 'gwmed_dia',\n",
    "                          'GWh acum. no Mês até o Dia': 'gwmed_mes',\n",
    "                          'GWh acum. no Ano até o Dia': 'gwmed_ano'})\n",
    "\n",
    "df_poth_rg = df_poth_rg.rename(columns={'Subsistema': 'subsistema',\n",
    "                          'MWmed no Dia': 'mwmed_dia',\n",
    "                          'MWmed no Mês até o Dia': 'mwmed_mes',\n",
    "                          'MWmed no Ano até o Dia': 'mwmed_ano'})\n",
    "\n",
    "df_pott_rg = df_pott_rg.rename(columns={'Subsistema': 'subsistema',\n",
    "                          'MWmed no Dia': 'mwmed_dia',\n",
    "                          'MWmed no Mês até o Dia': 'mwmed_mes',\n",
    "                          'MWmed no Ano até o Dia': 'mwmed_ano'})\n",
    "\n",
    "df_prodt_usina = df_prodt_usina.rename(columns={'Usina': 'usina',\n",
    "                          'Código ONS': 'codigo_ons',\n",
    "                          'Programado (MWmed)': 'programado',\n",
    "                          'Verificado (MWmed)': 'verificado'})\n",
    "\n",
    "df_prodh_usina = df_prodh_usina.rename(columns={'Usina': 'usina',\n",
    "                          'Código ONS': 'codigo_ons',\n",
    "                          'Programado (MWmed)': 'programado',\n",
    "                          'Verificado (MWmed)': 'verificado'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0e61a05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELT reservatorios\n",
    "import pandas as pd\n",
    "\n",
    "dfs = []\n",
    "for date in date_formats:\n",
    "    df_date = pd.DataFrame()\n",
    "    for regiao in ['sul','sudeste', 'nordeste', 'norte']:\n",
    "        cap_reservatorio = f'RES_D_{regiao.upper()}_{date}.xlsx'\n",
    "        if os.path.exists(cap_reservatorio):\n",
    "            df = pd.read_excel(cap_reservatorio, sheet_name='Plan1')\n",
    "            filename = os.path.basename(cap_reservatorio)\n",
    "            date_str = '_'.join(filename.split('_')[3:6]).split('.')[0]\n",
    "            df['data'] = date_str\n",
    "            df['regiao'] = regiao\n",
    "            df_date = pd.concat([df_date, df], axis=0)\n",
    "    dfs.append(df_date)\n",
    "\n",
    "# Concatena e remove col completamente vazias\n",
    "df_RES_D = pd.concat(dfs, axis=0)\n",
    "\n",
    "# Renomeia colunas\n",
    "df_RES_D = df_RES_D.rename(columns={'Unnamed: 0': 'bacia',\n",
    "                             'Unnamed: 1': 'reservatorio',\n",
    "                             'Unnamed: 2': 'del',\n",
    "                             'Unnamed: 3': 'nivel_m',\n",
    "                             'Unnamed: 4': 'vol_util_%',\n",
    "                             'Unnamed: 5': 'afluencia',\n",
    "                             'Unnamed: 6': 'defluencia',\n",
    "                             'Unnamed: 7': 'vertida',\n",
    "                             'Unnamed: 8': 'transferida'\n",
    "                                   })\n",
    "\n",
    "# remover valores duplicados na lista to_remove\n",
    "to_remove = list(set(['Dados Hidráulicos dos Reservatórios', 'Subsistema Sul', 'Subsistema Norte', 'Bacia', 'Subsistema Sul', 'Subsistema Nordeste', 'Subsistema Sudeste']))\n",
    "df_RES_D.drop('del', axis=1, inplace=True)\n",
    "\n",
    "# remover linhas do dataframe que contenham valores presentes em to_remove na coluna col_name\n",
    "df_RES_D = df_RES_D[~df_RES_D['bacia'].isin(to_remove)]\n",
    "df_RES_D = df_RES_D[df_RES_D['afluencia'] != 'Afluência']\n",
    "df_RES_D.drop_duplicates(subset=['reservatorio', 'data'], inplace=True)\n",
    "\n",
    "# Corrige falores da col 'bacia' que estavam mesclados no excel\n",
    "df_RES_D['bacia'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "#atualizando formato dos dados\n",
    "df_RES_D['nivel_m'] = pd.to_numeric(df_RES_D['nivel_m'], errors='coerce')\n",
    "df_RES_D['vol_util_%'] = pd.to_numeric(df_RES_D['vol_util_%'], errors='coerce')\n",
    "df_RES_D['afluencia'] = pd.to_numeric(df_RES_D['afluencia'], errors='coerce')\n",
    "df_RES_D['defluencia'] = pd.to_numeric(df_RES_D['defluencia'], errors='coerce')\n",
    "df_RES_D['vertida'] = pd.to_numeric(df_RES_D['vertida'], errors='coerce')\n",
    "df_RES_D['transferida'] = pd.to_numeric(df_RES_D['transferida'], errors='coerce')\n",
    "df_RES_D['data'] = pd.to_datetime(df_RES_D['data'],  format='%Y_%m_%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9058e08e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ad96b9a",
   "metadata": {},
   "source": [
    "Criando o banco de dados no PostgreSQL"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c24ea0da",
   "metadata": {},
   "source": [
    "# Cria tabelas no postgreSQL\n",
    "\n",
    "CREATE TABLE balanco_de_energia (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    data DATE NOT NULL,\n",
    "    total FLOAT NOT NULL,\n",
    "    hidraulica FLOAT NOT NULL,\n",
    "    termica FLOAT NOT NULL,\n",
    "    eolica FLOAT NOT NULL,\n",
    "    solar FLOAT NOT NULL,\n",
    "    intercambio FLOAT NOT NULL,\n",
    "    carga FLOAT NOT NULL,\n",
    "    regiao VARCHAR NOT NULL\n",
    ");\n",
    "\n",
    "\n",
    "CREATE TABLE usinas_termicas (\n",
    "    usina VARCHAR,\n",
    "    codigo_ons VARCHAR,\n",
    "    programado FLOAT,\n",
    "    verificado FLOAT,\n",
    "    data DATE\n",
    ");\n",
    "\n",
    "CREATE TABLE potencia_termicas (\n",
    "    subsistema VARCHAR NOT NULL,\n",
    "    mwmed_dia FLOAT NOT NULL,\n",
    "    mwmed_mes FLOAT NOT NULL,\n",
    "    mwmed_ano FLOAT NOT NULL,\n",
    "    data DATE NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE producao_termicas (\n",
    "   subsistema VARCHAR,\n",
    "   gwh_dia FLOAT,\n",
    "   gwh_mes FLOAT,\n",
    "   gwh_ano FLOAT,\n",
    "   data DATE\n",
    ");\n",
    "\n",
    "CREATE TABLE usinas_hidraulicas (\n",
    "    usina TEXT,\n",
    "    codigo_ons TEXT,\n",
    "    programado NUMERIC,\n",
    "    verificado NUMERIC,\n",
    "    data DATE\n",
    ");\n",
    "\n",
    "CREATE TABLE producao_hidraulicas (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    subsistema VARCHAR NOT NULL,\n",
    "    mwmed_dia FLOAT NOT NULL,\n",
    "    mwmed_mes FLOAT NOT NULL,\n",
    "    mwmed_ano FLOAT NOT NULL,\n",
    "    data DATE NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE potencia_hidraulicas (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    subsistema VARCHAR,\n",
    "    gwh_dia NUMERIC,\n",
    "    gwh_mes NUMERIC,\n",
    "    gwh_ano NUMERIC,\n",
    "    data DATE\n",
    ");\n",
    "\n",
    "CREATE TABLE reservatorios (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    bacia VARCHAR,\n",
    "    reservatorio VARCHAR,\n",
    "    nivel_m NUMERIC,\n",
    "    vol_util NUMERIC,\n",
    "    afluencia NUMERIC,\n",
    "    defluencia NUMERIC,\n",
    "    vertida NUMERIC,\n",
    "    transferida NUMERIC,\n",
    "    data DATE,\n",
    "    regiao VARCHAR\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1148cc2d",
   "metadata": {},
   "source": [
    "Operando no banco de dados criado - PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "34a2a82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conecta-se ao banco de dados local\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import psycopg2 as pg\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Cria conexão com o banco de dados por engine\n",
    "engine = create_engine('postgresql://postgres:gabriel123@localhost:5432/ONS_D')\n",
    "\n",
    "# Cria conexão com o banco por pycopg2\n",
    "connection = pg.connect(user = \"postgres\",\n",
    "                          password = \"gabriel123\",\n",
    "                          host = \"localhost\",\n",
    "                          port = \"5432\",\n",
    "                          database = \"ONS_D\")\n",
    "curs = connection.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aa2232",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Subindo dataframes gerados em Python para o banco ONS_D\n",
    "\n",
    "df_BEA_D.to_sql('balanco_de_energia', engine, if_exists='replace', index=False)\n",
    "\n",
    "df_pott_rg.to_sql('potencia_termicas', engine, if_exists='replace', index=False)\n",
    "df_poth_rg.to_sql('potencia_hidraulicas', engine, if_exists='replace', index=False)\n",
    "\n",
    "df_prodt_rg.to_sql('producao_termicas', engine, if_exists='replace', index=False)\n",
    "df_prodh_rg.to_sql('producao_hidraulicas', engine, if_exists='replace', index=False)\n",
    "\n",
    "df_prodh_usina.to_sql('usinas_hidraulicas', engine, if_exists='replace', index=False)\n",
    "df_prodt_usina.to_sql('usinas_termicas', engine, if_exists='replace', index=False)\n",
    "\n",
    "df_RES_D.to_sql('reservatorios', engine, if_exists='replace', index=False)\n",
    "\n",
    "\n",
    "\n",
    "# ETL no ONS_D  -  prevendo manipulações futuras realizei as operações individualmente\n",
    "\n",
    "# Remove linhas duplicadas no SQL - balanco_de_energia\n",
    "engine.execute(\"\"\"\n",
    "    DELETE FROM balanco_de_energia\n",
    "    WHERE ctid NOT IN (\n",
    "        SELECT DISTINCT ON (data, total, hidraulica, termica, eolica, solar, intercambio, carga, regiao) ctid\n",
    "        FROM balanco_de_energia\n",
    "        ORDER BY data, total, hidraulica, termica, eolica, solar, intercambio, carga, regiao, ctid\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Remove linhas duplicadas no SQL - potencia\n",
    "engine.execute(\"\"\"\n",
    "    DELETE FROM potencia_termicas\n",
    "    WHERE ctid NOT IN (\n",
    "        SELECT DISTINCT ON (subsistema, mwmed_dia) ctid\n",
    "        FROM potencia_termicas\n",
    "        WHERE subsistema != 'None'\n",
    "        ORDER BY subsistema, mwmed_dia, data, ctid\n",
    "    )\n",
    "\"\"\")\n",
    "engine.execute(\"\"\"\n",
    "    DELETE FROM potencia_hidraulicas\n",
    "    WHERE ctid NOT IN (\n",
    "        SELECT DISTINCT ON (subsistema, mwmed_dia) ctid\n",
    "        FROM potencia_hidraulicas\n",
    "        WHERE subsistema != 'None'\n",
    "        ORDER BY subsistema, mwmed_dia, data, ctid\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Remove linhas duplicadas no SQL - producao\n",
    "engine.execute(\"\"\"\n",
    "    DELETE FROM producao_termicas\n",
    "    WHERE ctid NOT IN (\n",
    "        SELECT DISTINCT ON (subsistema, gwmed_dia) ctid\n",
    "        FROM producao_termicas\n",
    "        WHERE subsistema != 'Subsistema'\n",
    "        ORDER BY subsistema, gwmed_dia, data, ctid\n",
    "    )\n",
    "\"\"\") \n",
    "engine.execute(\"\"\"\n",
    "    DELETE FROM producao_hidraulicas\n",
    "    WHERE ctid NOT IN (\n",
    "        SELECT DISTINCT ON (subsistema, gwmed_dia) ctid\n",
    "        FROM producao_hidraulicas\n",
    "        WHERE subsistema != 'Subsistema'\n",
    "        ORDER BY subsistema, gwmed_dia, data, ctid\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Remove linhas duplicadas no SQL - usinas\n",
    "engine.execute(\"\"\"\n",
    "    DELETE FROM usinas_hidraulicas\n",
    "    WHERE ctid NOT IN (\n",
    "        SELECT DISTINCT ON (codigo_ons, data) ctid\n",
    "        FROM usinas_hidraulicas\n",
    "        ORDER BY codigo_ons, data, ctid\n",
    "    )\n",
    "\"\"\") \n",
    "engine.execute(\"\"\"\n",
    "    DELETE FROM usinas_termicas\n",
    "    WHERE ctid NOT IN (\n",
    "        SELECT DISTINCT ON (codigo_ons, data) ctid\n",
    "        FROM usinas_termicas\n",
    "        ORDER BY codigo_ons, data, ctid\n",
    "    )\n",
    "\"\"\") \n",
    "\n",
    "# Remove linhas duplicadas no SQL - reservatorios\n",
    "engine.execute(\"\"\"\n",
    "    DELETE FROM reservatorios\n",
    "    WHERE ctid NOT IN (\n",
    "        SELECT DISTINCT ON (reservatorio, nivel_m) ctid\n",
    "        FROM reservatorios\n",
    "        WHERE reservatorio != 'None'\n",
    "        ORDER BY reservatorio, nivel_m, ctid\n",
    "    )\n",
    "\"\"\") \n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM reservatorios\", connection)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb154c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpa a pasta de download\n",
    "\n",
    "import os\n",
    "\n",
    "# Define o diretório onde os arquivos foram salvos\n",
    "diretorio = r'C:\\Users\\ghumb\\Desktop\\Python\\Projetos\\ONS'\n",
    "\n",
    "# Percorre todos os arquivos no diretório e exclui aqueles cujo nome começa com BDE, BEA ou RES e têm extensão .xlsx\n",
    "for filename in os.listdir(diretorio):\n",
    "    if (filename.startswith('BDE') or filename.startswith('BEA') or filename.startswith('RES')):\n",
    "        os.remove(os.path.join(diretorio, filename))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51e1056",
   "metadata": {},
   "outputs": [],
   "source": [
    "Agendador de taregas\n",
    "\n",
    "1 - Win+R\n",
    "2- taskschd.msc -> ok\n",
    "3- iniciar uma tarefa simples\n",
    "4- definir periodicidade\n",
    "5- indicar um arquivo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c46475",
   "metadata": {},
   "source": [
    "Balanço de Energia Acumulado no Mês Até o Dia -https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/02_BalancoEnergeticoAcumuloDia_05-03-2023.xlsx\n",
    "\n",
    "Balanço de Energia Diário- https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/01_RelBalancoEnergeticoDiario_05-03-2023.xlsx\n",
    "\n",
    "    \n",
    "DADOS ACUMULADO POR REGIÃO \n",
    "            sul - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/03_DadosDiariosAcumulados_Regiao_05-03-2023.xlsx\n",
    "\n",
    "            sudeste - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/04_DadosDiariosAcumulados_Regiao_05-03-2023.xlsx\n",
    "\n",
    "            nordeste - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/05_DadosDiariosAcumulados_Regiao_05-03-2023.xlsx\n",
    "\n",
    "            norte - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/06_DadosDiariosAcumulados_Regiao_05-03-2023.xlsx\n",
    "\n",
    "            sist. interligado nasc - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/07_DadosDiariosAcumulados_Regiao_05-03-2023.xlsx\n",
    "    \n",
    "    \n",
    "PRODUÇÃO\n",
    "\n",
    "            Produção hidraulica por usina - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/08_ProducaoHidraulicaUsina_05-03-2023.xlsx\n",
    "\n",
    "            produção eletrica por usina - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/09_ProducaoTermicaUsina_05-03-2023.xlsx\n",
    "\n",
    "            produção eolica (ta bichado, não libera o pdf no download)\n",
    "\n",
    "            produção solar  (ta bichado, não libera o pdf no download)\n",
    "\n",
    "            motivo do despacho termico - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/12_MotivoDespachoTermico_05-03-2023.xlsx\n",
    "\n",
    "            reserva girante da demanda máxima - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/13_ReservaGiranteDemandaMaxima_05-03-2023.xlsx\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "CARDA HORARIA\n",
    "\n",
    "carga horaria por subsistema - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/14_CargaHorariaSub_05-03-2023.xlsx\n",
    "\n",
    "carga diaria por subsistema - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/15_CargaDiariaSubmercado_05-03-2023.xlsx\n",
    "\n",
    "demanda máxima - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/16_DemandaMaxima_05-03-2023.xlsx\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "Integração de Novos Equipamentos e Linhas de Transmissão no SIN (??)\n",
    "\n",
    "\n",
    "Variação de Energia Armazenada - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/20_VariacaoCargaEnergiaArmazenada_05-03-2023.xlsx\n",
    "\n",
    "Energia natural afluente - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/21_EnergiaNaturalAfluente_05-03-2023.xlsx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DADOS HIDRAULICOS DOS RESERVATORIOS\n",
    "\n",
    "SUL - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/23_SituacaoPrincipaisReservatorios_Regiao_05-03-2023.xlsx\n",
    "\n",
    "SUDESTE - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/24_SituacaoPrincipaisReservatorios_Regiao_05-03-2023.xlsx\n",
    "\n",
    "NORDESTE - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/25_SituacaoPrincipaisReservatorios_Regiao_05-03-2023.xlsx\n",
    "\n",
    "NORTE - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/26_SituacaoPrincipaisReservatorios_Regiao_05-03-2023.xlsx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876bed7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
