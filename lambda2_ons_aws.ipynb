{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa5f532b",
   "metadata": {},
   "source": [
    "O arquivo lambda2_ons_aws.inpy tem como objetivos:\n",
    "\n",
    "1- Ler arquivos do bucket \"s3lambda\" de forma temporaria\n",
    "\n",
    "2- Tratar dados para armazenamento em um banco de dados\n",
    "\n",
    "3- Envia os arquivos para o S3 buncket \"etl_ons\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fd9f80",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615e5589",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from io import StringIO\n",
    "from io import BytesIO\n",
    "import shutil\n",
    "import fnmatch\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    # Nome do bucket S3 e diretório a ser pesquisado\n",
    "    bucket_name = 's3lambdaons'\n",
    "    prefix_BEA = 'BEA/'\n",
    "\n",
    "    # Lista todos os objetos no diretório do bucket\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix_BEA)\n",
    "    \n",
    "    # Cria uma lista vazia para armazenar os dataframes lidos\n",
    "    dfs_BEA = []\n",
    "    # Loop pelos objetos encontrados\n",
    "    for obj in response.get('Contents', []):\n",
    "        # Pula objetos que não são arquivos Excel\n",
    "        if not obj['Key'].endswith('.xlsx'):\n",
    "            continue\n",
    "        # Baixa o arquivo para um diretório temporário\n",
    "        tmp_dir = '/tmp/data'\n",
    "        os.makedirs(tmp_dir, exist_ok=True)\n",
    "        file_path = os.path.join(tmp_dir, os.path.basename(obj['Key']))\n",
    "        with open(file_path, 'wb') as f:\n",
    "            s3.download_fileobj(bucket_name, obj['Key'], f)\n",
    "            \n",
    "    # Lê o arquivo Excel como um dataframe\n",
    "        df = pd.read_excel(file_path, sheet_name='Plan1')\n",
    "    # Adiciona a coluna \"regiao\" com o nome da região extraído do nome do arquivo\n",
    "        regiao = obj['Key'].split('/')[1].split('_')[2].lower()\n",
    "        df['regiao'] = regiao\n",
    "    # Adiciona o dataframe à lista\n",
    "        dfs_BEA.append(df)\n",
    "\n",
    "        \n",
    "#ETL BEA   \n",
    "    df_BEA_D = pd.concat(dfs_BEA, axis=0)\n",
    "    # Renomeia colunas\n",
    "    df_BEA_D = df_BEA_D.rename(columns={'Unnamed: 0': 'data',\n",
    "                                 'Unnamed: 1': 'total',\n",
    "                                 'Unnamed: 2': 'hidraulica',\n",
    "                                 'Unnamed: 3': 'termica',\n",
    "                                 'Unnamed: 4': 'eolica',\n",
    "                                 'Unnamed: 5': 'solar',\n",
    "                                 'Unnamed: 6': 'intercambio',\n",
    "                                 'Unnamed: 7': 'carga'})\n",
    "    \n",
    "    to_remove = ['Dados Diários acumulados', 'Valores - MWmed', 'Subsistema Norte', 'Total', np.nan,'Data','Sistema Interligado Nacional', 'Subsistema Sul', 'Subsistema Nordeste', 'Subsistema Sudeste']\n",
    "    df_BEA_D = df_BEA_D[~df_BEA_D['data'].isin(to_remove)]       \n",
    "    df_BEA_D.dropna(subset=['total'], inplace=True)\n",
    "    \n",
    "    # Remove valores que se repetem em 'data' e 'regiao', incluindo valores vazios\n",
    "    df_BEA_D.drop_duplicates(subset=['data', 'regiao'], inplace=True)  \n",
    "    \n",
    "    #atualizando formato dos dados\n",
    "    df_BEA_D['total'] = pd.to_numeric(df_BEA_D['total'], errors='coerce')\n",
    "    df_BEA_D['hidraulica'] = pd.to_numeric(df_BEA_D['hidraulica'], errors='coerce')\n",
    "    df_BEA_D['termica'] = pd.to_numeric(df_BEA_D['termica'], errors='coerce')\n",
    "    df_BEA_D['eolica'] = pd.to_numeric(df_BEA_D['eolica'], errors='coerce')\n",
    "    df_BEA_D['solar'] = pd.to_numeric(df_BEA_D['solar'], errors='coerce')\n",
    "    df_BEA_D['intercambio'] = pd.to_numeric(df_BEA_D['intercambio'], errors='coerce')\n",
    "    df_BEA_D['carga'] = pd.to_numeric(df_BEA_D['carga'], errors='coerce')\n",
    "    #df_BEA_D['data'] = pd.to_datetime(df_BEA_D['data']')\n",
    "\n",
    "# Envia df_BEA_D para o bucket etl-ons\n",
    "    bucket_name2 = 'etl-ons'\n",
    "    prefix_bea_etl = 'BEA_etl/'\n",
    "        \n",
    "    # Exporta o dataframe como um arquivo Excel\n",
    "    excel_buffer = BytesIO()\n",
    "    df_BEA_D.to_excel(excel_buffer, index=False)\n",
    "                \n",
    "    # Faz o upload do arquivo Excel para o bucket S3\n",
    "    file_name = 'df_BEA_D.xlsx'\n",
    "    file_path = os.path.join('/tmp', file_name)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        f.write(excel_buffer.getvalue())\n",
    "    s3.upload_file(file_path, bucket_name2, f'{prefix_bea_etl}{file_name}')\n",
    "          \n",
    "          \n",
    " \n",
    "    # PROD          \n",
    "              \n",
    "    dfs_h1 = []\n",
    "    dfs_h2 = []\n",
    "    dfs_h3 = []\n",
    "    dfs_t1 = []\n",
    "    dfs_t2 = []\n",
    "    dfs_t3 = []\n",
    "    \n",
    "    # Define o nome do bucket e o prefixo da pasta\n",
    "    prefix_prod = 'PROD_D/'\n",
    "    filename_pattern_h = 'BDE_D_PROD__H*.xlsx'\n",
    "    \n",
    "    # Lista os objetos no bucket que correspondem ao prefixo\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix_prod)\n",
    "    \n",
    "    for obj in response['Contents']:\n",
    "        # Extrai o nome do arquivo do objeto\n",
    "        filename = obj['Key'].split('/')[-1]\n",
    "        \n",
    "        # Verifica se o nome do arquivo corresponde ao padrão\n",
    "        if fnmatch.fnmatch(filename, filename_pattern_h):\n",
    "            try:\n",
    "                # Extrai a data do nome do arquivo\n",
    "                date_str = '_'.join(filename.split('_')[5:9]).split('.')[0]\n",
    "                \n",
    "                # Tenta converter a string em um objeto datetime\n",
    "                try:\n",
    "                    date_obj = datetime.strptime(date_str, '%Y_%m_%d')\n",
    "                    \n",
    "                    # Lê as tabelas prodh_rg, poth_rg e prodh_usina em seus respectivos dataframes e adiciona uma coluna com a data\n",
    "                    obj = s3.get_object(Bucket=bucket_name, Key=obj['Key'])\n",
    "                    data = obj['Body'].read()\n",
    "                    prodh_rg = pd.read_excel(BytesIO(data), sheet_name='Plan1', header=3, usecols='A:D', nrows=7)\n",
    "                    prodh_rg['data'] = date_obj\n",
    "                    dfs_h1.append(prodh_rg)\n",
    "                    \n",
    "                    poth_rg = pd.read_excel(BytesIO(data), sheet_name='Plan1', header=11, usecols='A:D', nrows=7)\n",
    "                    poth_rg['data'] = date_obj\n",
    "                    dfs_h2.append(poth_rg)\n",
    "                    \n",
    "                    prodh_usina = pd.read_excel(BytesIO(data), sheet_name='Plan1', header=22, usecols='A:D', nrows=1000)\n",
    "                    prodh_usina['data'] = date_obj\n",
    "                    dfs_h3.append(prodh_usina)\n",
    "                except ValueError:\n",
    "                    # A string não pode ser convertida em uma data\n",
    "                    print(f\"Valor inválido na coluna date do arquivo {filename}: {date_str}\")\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao ler o arquivo {filename}: {e}\")\n",
    "     \n",
    "    \n",
    "    df_prodh_rg = pd.concat(dfs_h1)\n",
    "    df_poth_rg = pd.concat(dfs_h2)\n",
    "    df_prodh_usina = pd.concat(dfs_h3)\n",
    "    \n",
    "    # Envia os dataframes para o bucket etl_ons\n",
    "    prefix_PROD_etl = 'PROD/'\n",
    "    \n",
    "    # Exporta os dataframes como arquivos Excel\n",
    "    excel_buffers = []\n",
    "    \n",
    "    for i, df in enumerate([df_prodh_rg, df_poth_rg, df_prodh_usina]):\n",
    "        excel_buffer = BytesIO()\n",
    "        df.to_excel(excel_buffer, index=False)\n",
    "        excel_buffers.append(excel_buffer)\n",
    "        file_name = f'df_{[\"prodh_rg\", \"poth_rg\", \"prodh_usina\"][i]}.xlsx'\n",
    "        file_path = os.path.join('/tmp', file_name)\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(excel_buffer.getvalue())\n",
    "        s3.upload_file(file_path, bucket_name2, f'{prefix_PROD_etl}{file_name}')\n",
    "        \n",
    "# PROD_T   \n",
    "    filename_pattern_t = 'BDE_D_PROD__T*.xlsx'\n",
    "    for obj in response['Contents']:\n",
    "        # Extrai o nome do arquivo do objeto\n",
    "        filename = obj['Key'].split('/')[-1]\n",
    "        \n",
    "        # Verifica se o nome do arquivo corresponde ao padrão\n",
    "        if fnmatch.fnmatch(filename, filename_pattern_t):\n",
    "            try:\n",
    "                # Extrai a data do nome do arquivo\n",
    "                date_str = '_'.join(filename.split('_')[5:9]).split('.')[0]\n",
    "                \n",
    "                # Tenta converter a string em um objeto datetime\n",
    "                try:\n",
    "                    date_obj = datetime.strptime(date_str, '%Y_%m_%d')\n",
    "                    \n",
    "                    # Lê as tabelas prodh_rg, poth_rg e prodh_usina em seus respectivos dataframes e adiciona uma coluna com a data\n",
    "                    obj = s3.get_object(Bucket=bucket_name, Key=obj['Key'])\n",
    "                    data = obj['Body'].read()\n",
    "                    prodt_rg = pd.read_excel(BytesIO(data), sheet_name='Plan1', header=3, usecols='A:D', nrows=7)\n",
    "                    prodt_rg['data'] = date_obj\n",
    "                    dfs_t1.append(prodt_rg)\n",
    "                    \n",
    "                    pott_rg = pd.read_excel(BytesIO(data), sheet_name='Plan1', header=11, usecols='A:D', nrows=7)\n",
    "                    pott_rg['data'] = date_obj\n",
    "                    dfs_t2.append(pott_rg)\n",
    "                    \n",
    "                    prodt_usina = pd.read_excel(BytesIO(data), sheet_name='Plan1', header=22, usecols='A:D', nrows=1000)\n",
    "                    prodt_usina['data'] = date_obj\n",
    "                    dfs_t3.append(prodt_usina)\n",
    "                except ValueError:\n",
    "                    # A string não pode ser convertida em uma data\n",
    "                    print(f\"Valor inválido na coluna date do arquivo {filename}: {date_str}\")\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao ler o arquivo {filename}: {e}\")\n",
    "                    \n",
    "    df_prodt_rg = pd.concat(dfs_t1)\n",
    "    df_pott_rg = pd.concat(dfs_t2)\n",
    "    df_prodt_usina = pd.concat(dfs_t3)\n",
    "    \n",
    "# ETL prod, pot e usina \n",
    "    # A tabela df_prodt_rg apresentou valores em object mesmo a leitura correta\n",
    "    df_prodt_rg['GWh no Dia'] = pd.to_numeric(df_prodt_rg['GWh no Dia'], errors='coerce')\n",
    "    df_prodt_rg['GWh acum. no Mês até o Dia'] = pd.to_numeric(df_prodt_rg['GWh acum. no Mês até o Dia'], errors='coerce')\n",
    "    df_prodt_rg['GWh acum. no Ano até o Dia'] = pd.to_numeric(df_prodt_rg['GWh acum. no Ano até o Dia'], errors='coerce')\n",
    "    \n",
    "    \n",
    "    # Removendo linhas duplicadas\n",
    "    dfs_prod = [df_prodh_rg, df_poth_rg, df_prodh_usina, df_prodt_rg, df_pott_rg, df_prodt_usina]\n",
    "    dfs_prod = list(map(lambda df: df.drop_duplicates(), dfs_prod))\n",
    "    \n",
    "    \n",
    "    # Normatizando nome das colunas para criar relações no PostgreSQL\n",
    "    df_prodh_rg = df_prodh_rg.rename(columns={'Subsistema': 'subsistema',\n",
    "                              'GWh no Dia': 'gwmed_dia',\n",
    "                              'GWh acum. no Mês até o Dia': 'gwmed_mes',\n",
    "                              'GWh acum. no Ano até o Dia': 'gwmed_ano'})\n",
    "    \n",
    "    df_prodt_rg = df_prodt_rg.rename(columns={'Subsistema': 'subsistema',\n",
    "                              'GWh no Dia': 'gwmed_dia',\n",
    "                              'GWh acum. no Mês até o Dia': 'gwmed_mes',\n",
    "                              'GWh acum. no Ano até o Dia': 'gwmed_ano'})\n",
    "    \n",
    "    df_poth_rg = df_poth_rg.rename(columns={'Subsistema': 'subsistema',\n",
    "                              'MWmed no Dia': 'mwmed_dia',\n",
    "                              'MWmed no Mês até o Dia': 'mwmed_mes',\n",
    "                              'MWmed no Ano até o Dia': 'mwmed_ano'})\n",
    "    \n",
    "    df_pott_rg = df_pott_rg.rename(columns={'Subsistema': 'subsistema',\n",
    "                              'MWmed no Dia': 'mwmed_dia',\n",
    "                              'MWmed no Mês até o Dia': 'mwmed_mes',\n",
    "                              'MWmed no Ano até o Dia': 'mwmed_ano'})\n",
    "    \n",
    "    df_prodt_usina = df_prodt_usina.rename(columns={'Usina': 'usina',\n",
    "                              'Código ONS': 'codigo_ons',\n",
    "                              'Programado (MWmed)': 'programado',\n",
    "                              'Verificado (MWmed)': 'verificado'})\n",
    "    \n",
    "    df_prodh_usina = df_prodh_usina.rename(columns={'Usina': 'usina',\n",
    "                              'Código ONS': 'codigo_ons',\n",
    "                              'Programado (MWmed)': 'programado',\n",
    "                              'Verificado (MWmed)': 'verificado'})\n",
    "\n",
    "    # Exporta os dataframes como arquivos Excel\n",
    "    excel_buffers = []\n",
    "    for i, df in enumerate([df_prodt_rg, df_pott_rg, df_prodt_usina]):\n",
    "        excel_buffer = BytesIO()\n",
    "        df.to_excel(excel_buffer, index=False)\n",
    "        excel_buffers.append(excel_buffer)\n",
    "        file_name = f'df_{[\"prodt_rg\", \"pott_rg\", \"prodt_usina\"][i]}.xlsx'\n",
    "        file_path = os.path.join('/tmp', file_name)\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(excel_buffer.getvalue())\n",
    "        s3.upload_file(file_path, bucket_name2, f'{prefix_PROD_etl}{file_name}')          \n",
    "\n",
    "\n",
    "\n",
    "# RESERVATORIOS\n",
    "    # Lista todos os objetos no diretório do bucket\n",
    "    prefix_res = 'RES_D/'\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix_res)\n",
    "    \n",
    "    # Cria uma lista vazia para armazenar os dataframes lidos\n",
    "    dfs_res = []\n",
    "\n",
    "    # Loop pelos objetos encontrados\n",
    "    for obj in response.get('Contents', []):\n",
    "        # Pula objetos que não são arquivos Excel\n",
    "        if not obj['Key'].endswith('.xlsx'):\n",
    "            continue\n",
    "        # Baixa o arquivo para um diretório temporário\n",
    "        tmp_dir = '/tmp/data'\n",
    "        os.makedirs(tmp_dir, exist_ok=True)\n",
    "        file_path = os.path.join(tmp_dir, os.path.basename(obj['Key']))\n",
    "        with open(file_path, 'wb') as f:\n",
    "            s3.download_fileobj(bucket_name, obj['Key'], f)\n",
    "\n",
    "        # Lê o arquivo Excel como um dataframe\n",
    "        df = pd.read_excel(file_path, sheet_name='Plan1')\n",
    "        # Adiciona a coluna \"regiao\" com o nome da região extraído do nome do arquivo\n",
    "        regiao = obj['Key'].split('/')[1].split('_')[2].lower()\n",
    "        date_str = '_'.join(file_path.split('_')[3:6]).split('.')[0]\n",
    "        df['regiao'] = regiao\n",
    "        df['data'] = date_str\n",
    "\n",
    "        # Adiciona o dataframe à lista\n",
    "        dfs_res.append(df)\n",
    "\n",
    "    # Concatena todos os dataframes da lista em um único dataframe\n",
    "    df_RES_D = pd.concat(dfs_res, ignore_index=True)\n",
    "   \n",
    "   \n",
    "# ELT\n",
    "    # Renomeia colunas\n",
    "    df_RES_D = df_RES_D.rename(columns={'Unnamed: 0': 'bacia',\n",
    "                                 'Unnamed: 1': 'reservatorio',\n",
    "                                 'Unnamed: 2': 'del',\n",
    "                                 'Unnamed: 3': 'nivel_m',\n",
    "                                 'Unnamed: 4': 'vol_util_%',\n",
    "                                 'Unnamed: 5': 'afluencia',\n",
    "                                 'Unnamed: 6': 'defluencia',\n",
    "                                 'Unnamed: 7': 'vertida',\n",
    "                                 'Unnamed: 8': 'transferida'\n",
    "                                       })\n",
    "    \n",
    "    # remover valores duplicados na lista to_remove\n",
    "    to_remove = list(set(['Dados Hidráulicos dos Reservatórios', 'Subsistema Sul', 'Subsistema Norte', 'Bacia', 'Subsistema Sul', 'Subsistema Nordeste', 'Subsistema Sudeste']))\n",
    "    df_RES_D.drop('del', axis=1, inplace=True)\n",
    "    \n",
    "    # remover linhas do dataframe que contenham valores presentes em to_remove na coluna col_name\n",
    "    df_RES_D = df_RES_D[~df_RES_D['bacia'].isin(to_remove)]\n",
    "    df_RES_D.dropna(subset=['reservatorio'], inplace=True)\n",
    "    df_RES_D = df_RES_D[df_RES_D['afluencia'] != 'Afluência']\n",
    "    df_RES_D.drop_duplicates(subset=['reservatorio', 'data'], inplace=True)\n",
    "    \n",
    "    # Corrige falores da col 'bacia' que estavam mesclados no excel\n",
    "    df_RES_D['bacia'].fillna(method='ffill', inplace=True)\n",
    "    \n",
    "    #atualizando formato dos dados\n",
    "    df_RES_D['nivel_m'] = pd.to_numeric(df_RES_D['nivel_m'], errors='coerce')\n",
    "    df_RES_D['vol_util_%'] = pd.to_numeric(df_RES_D['vol_util_%'], errors='coerce')\n",
    "    df_RES_D['afluencia'] = pd.to_numeric(df_RES_D['afluencia'], errors='coerce')\n",
    "    df_RES_D['defluencia'] = pd.to_numeric(df_RES_D['defluencia'], errors='coerce')\n",
    "    df_RES_D['vertida'] = pd.to_numeric(df_RES_D['vertida'], errors='coerce')\n",
    "#    df_RES_D['transferida'] = pd.to_numeric(df_RES_D['transferida'], errors='coerce')\n",
    "    df_RES_D['data'] = pd.to_datetime(df_RES_D['data'],  format='%Y_%m_%d')\n",
    "   \n",
    "   \n",
    "\n",
    "# Envia df_BEA_D para o bucket etl-ons\n",
    "    bucket_name2 = 'etl-ons'\n",
    "    prefix_RES_etl = 'RESD/'\n",
    "        \n",
    "    # Exporta o dataframe como um arquivo Excel\n",
    "    excel_buffer = BytesIO()\n",
    "    df_RES_D.to_excel(excel_buffer, index=False)\n",
    "                \n",
    "    # Faz o upload do arquivo Excel para o bucket S3\n",
    "    file_name = 'df_RES_D.xlsx'\n",
    "    file_path = os.path.join('/tmp', file_name)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        f.write(excel_buffer.getvalue())\n",
    "    s3.upload_file(file_path, bucket_name2, f'{prefix_RES_etl}{file_name}')\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': 'sucesso'\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
