{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb748e11",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3672d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabelas disponíveis\n",
    "\n",
    "    # BALANÇO DE ENERGIA ACUMULADA DIÁRIA\n",
    "        df_BEA_D\n",
    "\n",
    "    # PRODUÇÃO DE ENERGIA POR TIPO\n",
    "        # Hidráulica\n",
    "        df_prodh_rg - produção diária por região + itaipu\n",
    "        df_poth_rg - potencia diária por região + itaipu\n",
    "        df_prodh_usina - produção e potencia diária por usina + itaipu\n",
    "        # Térmica\n",
    "        df_prodt_rg - produção diária por região\n",
    "        df_pott_rg - potencia diária por região\n",
    "        df_prodt_usina -produção e potencia diária por unidade geradora\n",
    "        # Eólica\n",
    "        # Solar\n",
    "    \n",
    "    # Dados dos reservatórios (incompleto)\n",
    "        df_RES_D\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da4512",
   "metadata": {},
   "source": [
    "Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0a9b52c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import fnmatch\n",
    "import urllib.request\n",
    "import telegram\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25abc23c",
   "metadata": {},
   "source": [
    "Download de tabelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "33bcf06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ghumb\\AppData\\Local\\Temp\\ipykernel_23100\\2564453714.py:47: RuntimeWarning: coroutine 'Bot.send_message' was never awaited\n",
      "  bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} baixado com sucesso')\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\ghumb\\AppData\\Local\\Temp\\ipykernel_23100\\2564453714.py:50: RuntimeWarning: coroutine 'Bot.send_message' was never awaited\n",
      "  bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} não encontrado')\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\ghumb\\AppData\\Local\\Temp\\ipykernel_23100\\2564453714.py:60: RuntimeWarning: coroutine 'Bot.send_message' was never awaited\n",
      "  bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de produção {prod_type} da data {data} baixado com sucesso')\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\ghumb\\AppData\\Local\\Temp\\ipykernel_23100\\2564453714.py:63: RuntimeWarning: coroutine 'Bot.send_message' was never awaited\n",
      "  bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de produção {prod_type} da data {data} não encontrado')\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\ghumb\\AppData\\Local\\Temp\\ipykernel_23100\\2564453714.py:73: RuntimeWarning: coroutine 'Bot.send_message' was never awaited\n",
      "  bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} baixado com sucesso')\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\ghumb\\AppData\\Local\\Temp\\ipykernel_23100\\2564453714.py:76: RuntimeWarning: coroutine 'Bot.send_message' was never awaited\n",
      "  bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} não encontrado')\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\ghumb\\AppData\\Local\\Temp\\ipykernel_23100\\2564453714.py:86: RuntimeWarning: coroutine 'Bot.send_message' was never awaited\n",
      "  bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} baixado com sucesso')\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\ghumb\\AppData\\Local\\Temp\\ipykernel_23100\\2564453714.py:89: RuntimeWarning: coroutine 'Bot.send_message' was never awaited\n",
      "  bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} não encontrado')\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define o número de dias anteriores para buscar as URLs das regiões\n",
    "num_days =3\n",
    "\n",
    "# Obtém as datas dos últimos 'num_days' dias e inverte a lista para buscar da menor para a maior data\n",
    "dates = [datetime.today() - timedelta(days=x) for x in range(num_days)][::-1]\n",
    "\n",
    "# Define os formatos das datas para uso na URL e no nome do arquivo\n",
    "date_formats = [date.strftime('%Y_%m_%d') for date in dates]\n",
    "date_formats2 = [date.strftime('%d-%m-%Y') for date in dates]\n",
    "\n",
    "# Define as URLs de download com variação das datas definidas\n",
    "url_regioes = {}\n",
    "url_prod = {}\n",
    "url_reservatorios= {}\n",
    "\n",
    "for i in range(num_days):\n",
    "    # BALANÇO DE ENERGIA ACUMULADA DIÁRIA - BEA\n",
    "    url_regioes[date_formats[i]] = {\n",
    "        'sul': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/03_DadosDiariosAcumulados_Regiao_{date_formats2[i]}.xlsx',\n",
    "        'sudeste': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/04_DadosDiariosAcumulados_Regiao_{date_formats2[i]}.xlsx',\n",
    "        'nordeste': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/05_DadosDiariosAcumulados_Regiao_{date_formats2[i]}.xlsx',\n",
    "        'norte': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/06_DadosDiariosAcumulados_Regiao_{date_formats2[i]}.xlsx',\n",
    "        'brasil': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/07_DadosDiariosAcumulados_Regiao_{date_formats2[i]}.xlsx'\n",
    "    }\n",
    "    # PRODUÇÃO DE ENERGIA POR TIPO (BDE_D)\n",
    "    url_prod[date_formats[i]] = {\n",
    "        '_H': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/08_ProducaoHidraulicaUsina_{date_formats2[i]}.xlsx',\n",
    "        '_T': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/09_ProducaoTermicaUsina_{date_formats2[i]}.xlsx'\n",
    "        #'PROD_S':PLATAFORMA QUEBRADA\n",
    "        #'PROD_E': PLATAFORMA QUEBRADA\n",
    "    }\n",
    "    \n",
    "    url_reservatorios[date_formats[i]] = {\n",
    "        'sul': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/23_SituacaoPrincipaisReservatorios_Regiao_{date_formats2[i]}.xlsx',\n",
    "        'sudeste': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/24_SituacaoPrincipaisReservatorios_Regiao_{date_formats2[i]}.xlsx',\n",
    "        'nordeste': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/25_SituacaoPrincipaisReservatorios_Regiao_{date_formats2[i]}.xlsx',\n",
    "        'norte': f'https://sdro.ons.org.br/SDRO/DIARIO/{date_formats[i]}/HTML/26_SituacaoPrincipaisReservatorios_Regiao_{date_formats2[i]}.xlsx'\n",
    "    }\n",
    "    \n",
    "# Percorre URL_regioes para tentar baixar os arquivos\n",
    "for data, urls in url_regioes.items():\n",
    "    for regiao, url in urls.items():\n",
    "        filename = f'BEA_D_{regiao.upper()}_{data}.xlsx'\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "            bot = telegram.Bot(token='SEU_TOKEN_AQUI')\n",
    "            bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} baixado com sucesso')\n",
    "        except urllib.error.HTTPError:\n",
    "            bot = telegram.Bot(token='SEU_TOKEN_AQUI')\n",
    "            bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} não encontrado')\n",
    "            continue\n",
    "\n",
    "# Percorre URL_prod para tentar baixar os arquivos\n",
    "for data, urls in url_prod.items():\n",
    "    for prod_type, url in urls.items():\n",
    "        filename = f'BDE_D_PROD_{prod_type}_{data}.xlsx'\n",
    "        try:      \n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "            bot = telegram.Bot(token='SEU_TOKEN_AQUI')\n",
    "            bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de produção {prod_type} da data {data} baixado com sucesso')\n",
    "        except urllib.error.HTTPError:\n",
    "            bot = telegram.Bot(token='SEU_TOKEN_AQUI')\n",
    "            bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de produção {prod_type} da data {data} não encontrado')\n",
    "            continue\n",
    "    \n",
    "# Percorre URL_regioes para tentar baixar os arquivos\n",
    "for data, urls in url_reservatorios.items():\n",
    "    for reservatorios, url in urls.items():\n",
    "        filename = f'RES_{regiao.upper()}_{data}.xlsx'\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "            bot = telegram.Bot(token='SEU_TOKEN_AQUI')\n",
    "            bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} baixado com sucesso')\n",
    "        except urllib.error.HTTPError:\n",
    "            bot = telegram.Bot(token='SEU_TOKEN_AQUI')\n",
    "            bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} não encontrado')\n",
    "            continue\n",
    "\n",
    "# Percorre URL_reservatorios para tentar baixar os arquivos\n",
    "for data, urls in url_reservatorios.items():\n",
    "    for regiao, url in urls.items():\n",
    "        filename = f'RES_D_{regiao.upper()}_{data}.xlsx'\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "            bot = telegram.Bot(token='SEU_TOKEN_AQUI')\n",
    "            bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} baixado com sucesso')\n",
    "        except urllib.error.HTTPError:\n",
    "            bot = telegram.Bot(token='SEU_TOKEN_AQUI')\n",
    "            bot.send_message(chat_id='SEU_CHAT_ID_AQUI', text=f'Arquivo de {regiao} da data {data} não encontrado')\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebc809b",
   "metadata": {},
   "source": [
    "ETLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebe08d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL BALANÇO DE ENERGIA ACUMULADA DIÁRIA\n",
    "\n",
    "dfs = []\n",
    "for date in date_formats:\n",
    "    df_date = pd.DataFrame()\n",
    "    for regiao in ['sul','sudeste', 'nordeste', 'norte','brasil']:\n",
    "        prod_regiao = f'BEA_D_{regiao.upper()}_{date}.xlsx'\n",
    "        if os.path.exists(prod_regiao):\n",
    "            df = pd.read_excel(prod_regiao, sheet_name='Plan1')\n",
    "            df['regiao'] = regiao\n",
    "            df_date = pd.concat([df_date, df], axis=0)\n",
    "        dfs.append(df_date)\n",
    "        \n",
    "\n",
    "# Concatena e remove col completamente vazias\n",
    "    df_BEA_D = pd.concat(dfs, axis=0).dropna()\n",
    "\n",
    "# Renomeia colunas\n",
    "    df_BEA_D = df_BEA_D.rename(columns={'Unnamed: 0': 'data',\n",
    "                             'Unnamed: 1': 'total',\n",
    "                             'Unnamed: 2': 'hidraulica',\n",
    "                             'Unnamed: 3': 'termica',\n",
    "                             'Unnamed: 4': 'eolica',\n",
    "                             'Unnamed: 5': 'solar',\n",
    "                             'Unnamed: 6': 'intercambio',\n",
    "                             'Unnamed: 7': 'carga'})\n",
    "\n",
    "# Limpando o dataframe\n",
    "    to_remove = ['Dados Diários acumulados', 'Valores - MWmed', 'Subsistema Norte', 'Total', np.nan,'Data', 'Subsistema Sul', 'Subsistema Nordeste', 'Subsistema Sudeste']\n",
    "    df_BEA_D = df_BEA_D[~df_BEA_D['data'].isin(to_remove)]\n",
    "\n",
    "# Remove valores que se repetem em 'data' e 'regiao', incluindo valores vazios\n",
    "    df_BEA_D.drop_duplicates(subset=['data', 'regiao'], inplace=True)\n",
    "\n",
    "#atualizando formato dos dados\n",
    "    df_BEA_D['total'] = pd.to_numeric(df_BEA_D['total'], errors='coerce')\n",
    "    df_BEA_D['hidraulica'] = pd.to_numeric(df_BEA_D['hidraulica'], errors='coerce')\n",
    "    df_BEA_D['termica'] = pd.to_numeric(df_BEA_D['termica'], errors='coerce')\n",
    "    df_BEA_D['eolica'] = pd.to_numeric(df_BEA_D['eolica'], errors='coerce')\n",
    "    df_BEA_D['solar'] = pd.to_numeric(df_BEA_D['solar'], errors='coerce')\n",
    "    df_BEA_D['intercambio'] = pd.to_numeric(df_BEA_D['intercambio'], errors='coerce')\n",
    "    df_BEA_D['carga'] = pd.to_numeric(df_BEA_D['carga'], errors='coerce')\n",
    "    df_BEA_D['data'] = pd.to_datetime(df_BEA_D['data'], format='%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "838dceeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subsistema</th>\n",
       "      <th>GWh no Dia</th>\n",
       "      <th>GWh acum. no Mês até o Dia</th>\n",
       "      <th>GWh acum. no Ano até o Dia</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Norte</td>\n",
       "      <td>259.20</td>\n",
       "      <td>1979.61</td>\n",
       "      <td>20068.51</td>\n",
       "      <td>2023-03-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nordeste</td>\n",
       "      <td>90.13</td>\n",
       "      <td>594.78</td>\n",
       "      <td>8063.61</td>\n",
       "      <td>2023-03-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sul</td>\n",
       "      <td>232.01</td>\n",
       "      <td>1467.10</td>\n",
       "      <td>11125.97</td>\n",
       "      <td>2023-03-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sudeste/Centro-Oeste</td>\n",
       "      <td>623.60</td>\n",
       "      <td>4299.25</td>\n",
       "      <td>39656.87</td>\n",
       "      <td>2023-03-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Itaipu</td>\n",
       "      <td>203.50</td>\n",
       "      <td>1367.04</td>\n",
       "      <td>10426.00</td>\n",
       "      <td>2023-03-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sistema Interligado Nacional</td>\n",
       "      <td>1408.44</td>\n",
       "      <td>9707.77</td>\n",
       "      <td>89340.97</td>\n",
       "      <td>2023-03-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Subsistema  GWh no Dia  GWh acum. no Mês até o Dia  \\\n",
       "0                         Norte      259.20                     1979.61   \n",
       "1                      Nordeste       90.13                      594.78   \n",
       "2                           Sul      232.01                     1467.10   \n",
       "3          Sudeste/Centro-Oeste      623.60                     4299.25   \n",
       "4                        Itaipu      203.50                     1367.04   \n",
       "5  Sistema Interligado Nacional     1408.44                     9707.77   \n",
       "6                           NaN         NaN                         NaN   \n",
       "\n",
       "   GWh acum. no Ano até o Dia       data  \n",
       "0                    20068.51 2023-03-07  \n",
       "1                     8063.61 2023-03-07  \n",
       "2                    11125.97 2023-03-07  \n",
       "3                    39656.87 2023-03-07  \n",
       "4                    10426.00 2023-03-07  \n",
       "5                    89340.97 2023-03-07  \n",
       "6                         NaN 2023-03-07  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# Obtém o caminho atual do diretório do script\n",
    "dir_path = os.getcwd()\n",
    "\n",
    "# Define o padrão de nome de arquivo para correspondência\n",
    "filename_pattern_h = 'BDE_D_PROD__H*.xlsx'\n",
    "filename_pattern_t = 'BDE_D_PROD__T*.xlsx'\n",
    "\n",
    "# Cria listas vazias para armazenar os DataFrames de cada tipo de arquivo\n",
    "dfs_h1 = []\n",
    "dfs_h2 = []\n",
    "dfs_h3 = []\n",
    "dfs_t1 = []\n",
    "dfs_t2 = []\n",
    "dfs_t3 = []\n",
    "\n",
    "# Percorre todos os arquivos na pasta atual que correspondem ao padrão de nome de arquivo\n",
    "for filename in os.listdir(dir_path):\n",
    "    if fnmatch.fnmatch(filename, filename_pattern_h):\n",
    "        try:\n",
    "            # Extrai a data do nome do arquivo\n",
    "            date_str = '_'.join(filename.split('_')[5:9]).split('.')[0]\n",
    "            \n",
    "            # Tenta converter a string em um objeto datetime\n",
    "            try:\n",
    "                date_obj = datetime.datetime.strptime(date_str, '%Y_%m_%d')\n",
    "                \n",
    "                # Lê as tabelas prodh_rg, poth_rg e prodh_usina em seus respectivos dataframes e adiciona uma coluna com a data\n",
    "                prodh_rg = pd.read_excel(os.path.join(dir_path, filename), sheet_name='Plan1', header=3, usecols='A:D', nrows=7)\n",
    "                prodh_rg['data'] = date_obj\n",
    "                dfs_h1.append(prodh_rg)\n",
    "                \n",
    "                poth_rg = pd.read_excel(os.path.join(dir_path, filename), sheet_name='Plan1', header=11, usecols='A:D', nrows=7)\n",
    "                poth_rg['data'] = date_obj\n",
    "                dfs_h2.append(poth_rg)\n",
    "                \n",
    "                prodh_usina = pd.read_excel(os.path.join(dir_path, filename), sheet_name='Plan1', header=22, usecols='A:D', nrows=1000)\n",
    "                prodh_usina['data'] = date_obj\n",
    "                dfs_h3.append(prodh_usina)\n",
    "            except ValueError:\n",
    "                # A string não pode ser convertida em uma data\n",
    "                print(f\"Valor inválido na coluna date do arquivo {filename}: {date_str}\")\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler o arquivo {filename}: {e}\")\n",
    "    \n",
    "    elif fnmatch.fnmatch(filename, filename_pattern_t):\n",
    "        try:\n",
    "            # Extrai a data do nome do arquivo\n",
    "            date_str = '_'.join(filename.split('_')[5:9]).split('.')[0]\n",
    "            \n",
    "            # Tenta converter a string em um objeto datetime\n",
    "            try:\n",
    "                date_obj = datetime.datetime.strptime(date_str, '%Y_%m_%d')\n",
    "                \n",
    "                # Lê as tabelas prodt_rg, pott_rg e prodt_usina em seus respectivos dataframes e adiciona uma coluna com a data\n",
    "                prodt_rg = pd.read_excel(os.path.join(dir_path, filename), sheet_name='Plan1', header=3, usecols='A:D', nrows=7)\n",
    "                prodt_rg['data'] = date_obj\n",
    "                dfs_t1.append(prodt_rg)\n",
    "                \n",
    "                pott_rg = pd.read_excel(os.path.join(dir_path, filename), sheet_name='Plan1', header=10, usecols='A:D', nrows=7)\n",
    "                pott_rg['data'] = date_obj\n",
    "                dfs_t2.append(pott_rg)\n",
    "                \n",
    "                prodt_usina = pd.read_excel(os.path.join(dir_path, filename), sheet_name='Plan1', header=20, usecols='A:D', nrows=1000)\n",
    "                prodt_usina['data'] = date_obj\n",
    "                dfs_t3.append(prodh_usina)\n",
    "            except ValueError:\n",
    "                # A string não pode ser convertida em uma data\n",
    "                print(f\"Valor inválido na coluna date do arquivo {filename}: {date_str}\")\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler o arquivo {filename}: {e}\")\n",
    "        \n",
    "# OBS: a tabela BDE_D_PROD__H Itaipu é classificada como uma região, criando +1 linha de diferença por tabela à BDE_D_PROD__T \n",
    "\n",
    "# Concatenando os dataframes Hidráulico Térmico\n",
    "df_prodh_rg = pd.concat(dfs_h1)\n",
    "df_poth_rg = pd.concat(dfs_h2)\n",
    "df_prodh_usina = pd.concat(dfs_h3)\n",
    "df_prodt_rg = pd.concat(dfs_t1)\n",
    "df_pott_rg = pd.concat(dfs_t2)\n",
    "df_prodt_usina = pd.concat(dfs_t3)\n",
    "\n",
    "\n",
    "# A tabela df_prodt_rg apresentou valores em object mesmo a leitura correta\n",
    "df_prodt_rg['GWh no Dia'] = pd.to_numeric(df_prodt_rg['GWh no Dia'], errors='coerce')\n",
    "df_prodt_rg['GWh acum. no Mês até o Dia'] = pd.to_numeric(df_prodt_rg['GWh acum. no Mês até o Dia'], errors='coerce')\n",
    "df_prodt_rg['GWh acum. no Ano até o Dia'] = pd.to_numeric(df_prodt_rg['GWh acum. no Ano até o Dia'], errors='coerce')\n",
    "\n",
    "\n",
    "# Removendo linhas duplicadas\n",
    "dfs = [df_prodh_rg, df_poth_rg, df_prodh_usina, df_prodt_rg, df_pott_rg, df_prodt_usina]\n",
    "dfs = list(map(lambda df: df.drop_duplicates(), dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0eb7a5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0e61a05b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bacia</th>\n",
       "      <th>reservatorio</th>\n",
       "      <th>nivel_m</th>\n",
       "      <th>vol_util_%</th>\n",
       "      <th>afluencia</th>\n",
       "      <th>defluencia</th>\n",
       "      <th>vertida</th>\n",
       "      <th>transferida</th>\n",
       "      <th>data</th>\n",
       "      <th>regiao</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>sul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CAPIVARI</td>\n",
       "      <td>G. P. SOUZA</td>\n",
       "      <td>844.919983</td>\n",
       "      <td>99.290001</td>\n",
       "      <td>32.610001</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>sul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ITAJAI</td>\n",
       "      <td>SALTO PILAO</td>\n",
       "      <td>319.540009</td>\n",
       "      <td>162.979996</td>\n",
       "      <td>170.250000</td>\n",
       "      <td>170.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>sul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>IGUACU</td>\n",
       "      <td>G. B. MUNHOZ</td>\n",
       "      <td>741.890015</td>\n",
       "      <td>99.610001</td>\n",
       "      <td>856.330017</td>\n",
       "      <td>998.0</td>\n",
       "      <td>461.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>sul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>IGUACU</td>\n",
       "      <td>SEGREDO</td>\n",
       "      <td>606.849976</td>\n",
       "      <td>96.900002</td>\n",
       "      <td>1236.599976</td>\n",
       "      <td>1255.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>-123.000000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>sul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AMAZONAS</td>\n",
       "      <td>STO ANTONIO DO JARI</td>\n",
       "      <td>30.799999</td>\n",
       "      <td>203.960007</td>\n",
       "      <td>2867.000000</td>\n",
       "      <td>2867.0</td>\n",
       "      <td>2063.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>norte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AMAZONAS</td>\n",
       "      <td>BELO MONTE</td>\n",
       "      <td>95.760002</td>\n",
       "      <td>45.180000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8619.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>norte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AMAZONAS</td>\n",
       "      <td>PIMENTAL</td>\n",
       "      <td>96.930000</td>\n",
       "      <td>-13.670000</td>\n",
       "      <td>15690.980469</td>\n",
       "      <td>6651.0</td>\n",
       "      <td>4307.0</td>\n",
       "      <td>8957.209961</td>\n",
       "      <td>NaT</td>\n",
       "      <td>norte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TOCANTINS</td>\n",
       "      <td>ESTREITO</td>\n",
       "      <td>154.429993</td>\n",
       "      <td>65.580002</td>\n",
       "      <td>5012.080078</td>\n",
       "      <td>4223.0</td>\n",
       "      <td>2884.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>norte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TOCANTINS</td>\n",
       "      <td>TUCURUI</td>\n",
       "      <td>73.919998</td>\n",
       "      <td>99.379997</td>\n",
       "      <td>17640.710938</td>\n",
       "      <td>16948.0</td>\n",
       "      <td>12383.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>norte</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        bacia         reservatorio     nivel_m  vol_util_%     afluencia  \\\n",
       "1         NaN                  NaN         NaN         NaN           NaN   \n",
       "6    CAPIVARI          G. P. SOUZA  844.919983   99.290001     32.610001   \n",
       "7      ITAJAI          SALTO PILAO  319.540009  162.979996    170.250000   \n",
       "8      IGUACU         G. B. MUNHOZ  741.890015   99.610001    856.330017   \n",
       "9      IGUACU              SEGREDO  606.849976   96.900002   1236.599976   \n",
       "..        ...                  ...         ...         ...           ...   \n",
       "11   AMAZONAS  STO ANTONIO DO JARI   30.799999  203.960007   2867.000000   \n",
       "12   AMAZONAS           BELO MONTE   95.760002   45.180000      0.000000   \n",
       "13   AMAZONAS             PIMENTAL   96.930000  -13.670000  15690.980469   \n",
       "14  TOCANTINS             ESTREITO  154.429993   65.580002   5012.080078   \n",
       "15  TOCANTINS              TUCURUI   73.919998   99.379997  17640.710938   \n",
       "\n",
       "    defluencia  vertida  transferida data regiao  \n",
       "1          NaN      NaN          NaN  NaT    sul  \n",
       "6         23.0      0.0     0.000000  NaT    sul  \n",
       "7        170.0     59.0     0.000000  NaT    sul  \n",
       "8        998.0    461.0     0.000000  NaT    sul  \n",
       "9       1255.0    353.0  -123.000000  NaT    sul  \n",
       "..         ...      ...          ...  ...    ...  \n",
       "11      2867.0   2063.0     0.000000  NaT  norte  \n",
       "12      8619.0      0.0     0.000000  NaT  norte  \n",
       "13      6651.0   4307.0  8957.209961  NaT  norte  \n",
       "14      4223.0   2884.0     0.000000  NaT  norte  \n",
       "15     16948.0  12383.0     0.000000  NaT  norte  \n",
       "\n",
       "[168 rows x 10 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ELT reservatorios\n",
    "import pandas as pd\n",
    "\n",
    "dfs = []\n",
    "for date in date_formats:\n",
    "    df_date = pd.DataFrame()\n",
    "    for regiao in ['sul','sudeste', 'nordeste', 'norte']:\n",
    "        cap_reservatorio = f'RES_D_{regiao.upper()}_{date}.xlsx'\n",
    "        if os.path.exists(cap_reservatorio):\n",
    "            df = pd.read_excel(cap_reservatorio, sheet_name='Plan1')\n",
    "            filename = os.path.basename(cap_reservatorio)\n",
    "            date_str = '_'.join(filename.split('_')[3:6]).split('.')[0]\n",
    "            df['data'] = date_str\n",
    "            df['regiao'] = regiao\n",
    "            df_date = pd.concat([df_date, df], axis=0)\n",
    "    dfs.append(df_date)\n",
    "\n",
    "# Concatena e remove col completamente vazias\n",
    "df_RES_D = pd.concat(dfs, axis=0)\n",
    "\n",
    "# Renomeia colunas\n",
    "df_RES_D = df_RES_D.rename(columns={'Unnamed: 0': 'bacia',\n",
    "                             'Unnamed: 1': 'reservatorio',\n",
    "                             'Unnamed: 2': 'del',\n",
    "                             'Unnamed: 3': 'nivel_m',\n",
    "                             'Unnamed: 4': 'vol_util_%',\n",
    "                             'Unnamed: 5': 'afluencia',\n",
    "                             'Unnamed: 6': 'defluencia',\n",
    "                             'Unnamed: 7': 'vertida',\n",
    "                             'Unnamed: 8': 'transferida'\n",
    "                                   })\n",
    "\n",
    "# remover valores duplicados na lista to_remove\n",
    "to_remove = list(set(['Dados Hidráulicos dos Reservatórios', 'Subsistema Sul', 'Subsistema Norte', 'Bacia', 'Subsistema Sul', 'Subsistema Nordeste', 'Subsistema Sudeste']))\n",
    "df_RES_D.drop('del', axis=1, inplace=True)\n",
    "\n",
    "# remover linhas do dataframe que contenham valores presentes em to_remove na coluna col_name\n",
    "df_RES_D = df_RES_D[~df_RES_D['bacia'].isin(to_remove)]\n",
    "df_RES_D = df_RES_D[df_RES_D['afluencia'] != 'Afluência']\n",
    "df_RES_D.drop_duplicates(subset=['reservatorio', 'data'], inplace=True)\n",
    "\n",
    "# Corrige falores da col 'bacia' que estavam mesclados no excel\n",
    "df_RES_D['bacia'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "#atualizando formato dos dados\n",
    "df_RES_D['nivel_m'] = pd.to_numeric(df_RES_D['nivel_m'], errors='coerce')\n",
    "df_RES_D['vol_util_%'] = pd.to_numeric(df_RES_D['vol_util_%'], errors='coerce')\n",
    "df_RES_D['afluencia'] = pd.to_numeric(df_RES_D['afluencia'], errors='coerce')\n",
    "df_RES_D['defluencia'] = pd.to_numeric(df_RES_D['defluencia'], errors='coerce')\n",
    "df_RES_D['vertida'] = pd.to_numeric(df_RES_D['vertida'], errors='coerce')\n",
    "df_RES_D['transferida'] = pd.to_numeric(df_RES_D['transferida'], errors='coerce')\n",
    "df_RES_D['data'] = pd.to_datetime(df_RES_D['data'], errors='coerce')\n",
    "\n",
    "df_RES_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7918dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4119498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79c46475",
   "metadata": {},
   "source": [
    "Balanço de Energia Acumulado no Mês Até o Dia -https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/02_BalancoEnergeticoAcumuloDia_05-03-2023.xlsx\n",
    "\n",
    "Balanço de Energia Diário- https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/01_RelBalancoEnergeticoDiario_05-03-2023.xlsx\n",
    "\n",
    "    \n",
    "DADOS ACUMULADO POR REGIÃO \n",
    "            sul - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/03_DadosDiariosAcumulados_Regiao_05-03-2023.xlsx\n",
    "\n",
    "            sudeste - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/04_DadosDiariosAcumulados_Regiao_05-03-2023.xlsx\n",
    "\n",
    "            nordeste - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/05_DadosDiariosAcumulados_Regiao_05-03-2023.xlsx\n",
    "\n",
    "            norte - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/06_DadosDiariosAcumulados_Regiao_05-03-2023.xlsx\n",
    "\n",
    "            sist. interligado nasc - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/07_DadosDiariosAcumulados_Regiao_05-03-2023.xlsx\n",
    "    \n",
    "    \n",
    "PRODUÇÃO\n",
    "\n",
    "            Produção hidraulica por usina - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/08_ProducaoHidraulicaUsina_05-03-2023.xlsx\n",
    "\n",
    "            produção eletrica por usina - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/09_ProducaoTermicaUsina_05-03-2023.xlsx\n",
    "\n",
    "            produção eolica (ta bichado, não libera o pdf no download)\n",
    "\n",
    "            produção solar  (ta bichado, não libera o pdf no download)\n",
    "\n",
    "            motivo do despacho termico - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/12_MotivoDespachoTermico_05-03-2023.xlsx\n",
    "\n",
    "            reserva girante da demanda máxima - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/13_ReservaGiranteDemandaMaxima_05-03-2023.xlsx\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "CARDA HORARIA\n",
    "\n",
    "carga horaria por subsistema - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/14_CargaHorariaSub_05-03-2023.xlsx\n",
    "\n",
    "carga diaria por subsistema - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/15_CargaDiariaSubmercado_05-03-2023.xlsx\n",
    "\n",
    "demanda máxima - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/16_DemandaMaxima_05-03-2023.xlsx\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "Integração de Novos Equipamentos e Linhas de Transmissão no SIN (??)\n",
    "\n",
    "\n",
    "Variação de Energia Armazenada - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/20_VariacaoCargaEnergiaArmazenada_05-03-2023.xlsx\n",
    "\n",
    "Energia natural afluente - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/21_EnergiaNaturalAfluente_05-03-2023.xlsx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DADOS HIDRAULICOS DOS RESERVATORIOS\n",
    "\n",
    "SUL - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/23_SituacaoPrincipaisReservatorios_Regiao_05-03-2023.xlsx\n",
    "\n",
    "SUDESTE - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/24_SituacaoPrincipaisReservatorios_Regiao_05-03-2023.xlsx\n",
    "\n",
    "NORDESTE - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/25_SituacaoPrincipaisReservatorios_Regiao_05-03-2023.xlsx\n",
    "\n",
    "NORTE - https://sdro.ons.org.br/SDRO/DIARIO/2023_03_05/HTML/26_SituacaoPrincipaisReservatorios_Regiao_05-03-2023.xlsx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876bed7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
